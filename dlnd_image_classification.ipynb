{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 24 Max Value: 130\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 4 Name: deer\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njQl6AGjs/wCMj7S6AwR1rgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f318da7d9b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print([None,image_shape[0],image_shape[1],image_shape[2]])\n",
    "    tf_shape = [None,image_shape[0],image_shape[1],image_shape[2]]\n",
    "    return tf.placeholder(tf.float32, shape = tf_shape, name = 'x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print([None, n_classes])\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes], name = 'y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name = 'keep_prob') \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.get_shape())\n",
    "    #print(conv_num_outputs)\n",
    "    #print(conv_ksize)\n",
    "    #print(conv_strides)\n",
    "    #print(pool_ksize)\n",
    "    #print(pool_strides)\n",
    "\n",
    "    output_depth  = conv_num_outputs\n",
    "    input_depth   = x_tensor.get_shape()[3].value\n",
    "    \n",
    "    # weight & bias\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1],input_depth,output_depth],mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    #print(weight.get_shape()) ##debug\n",
    "    bias = tf.Variable(tf.truncated_normal([output_depth],mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    \n",
    "    conv_strides    = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    maxpool_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    maxpool_ksize   = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    \n",
    "    # convolution\n",
    "    conv = tf.nn.conv2d(x_tensor, weight, strides = conv_strides, padding = 'SAME')\n",
    "    #print(conv.get_shape()) ##debug\n",
    "    conv = tf.nn.bias_add(conv,bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    #max pooling\n",
    "    #print(conv.get_shape()) ##debug\n",
    "    max_pool = tf.nn.max_pool(conv, ksize = maxpool_ksize, strides = maxpool_strides, padding = 'SAME')\n",
    "    #print(max_pool.get_shape()) ##debug\n",
    "    return max_pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_tensor_dim = x_tensor.get_shape().as_list()[1:]\n",
    "    flattened_size = x_tensor_dim[0] * x_tensor_dim[1] * x_tensor_dim[2]   \n",
    "    \n",
    "    return tf.reshape(x_tensor, [-1, flattened_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.get_shape())\n",
    "    \n",
    "    # weight & bias\n",
    "    w = tf.Variable(tf.truncated_normal([x_tensor.get_shape()[1].value, num_outputs],mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    \n",
    "    fc = tf.add(tf.matmul(x_tensor,w),bias)\n",
    "    \n",
    "    return tf.nn.relu(fc)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.get_shape()[1].value)\n",
    "    \n",
    "    # weight & bias\n",
    "    weigth_out = tf.Variable(tf.truncated_normal([x_tensor.get_shape()[1].value, num_outputs],mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    bias_out = tf.Variable(tf.truncated_normal([num_outputs],mean=0.0, stddev=0.05, dtype=tf.float32))\n",
    "    \n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weigth_out), bias_out)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv_num_outputs = 32\n",
    "    conv_ksize = [3,3]\n",
    "    conv_strides = [1,1]\n",
    "    pool_ksize = [3,3]\n",
    "    pool_strides = [2,2]\n",
    "    \n",
    "    conv = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "    conv_num_outputs = 64\n",
    "    pool_strides = [1,1]\n",
    "    conv = conv2d_maxpool(conv, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv = conv2d_maxpool(conv, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    conv = flatten(conv)\n",
    "      \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:  \n",
    "    num_outputs = 512\n",
    "    fc = fully_conn(conv, num_outputs)\n",
    "    \n",
    "    num_outputs = 1024\n",
    "    fc = fully_conn(conv, num_outputs)\n",
    "    \n",
    "    fc = tf.nn.dropout(fc, keep_prob)\n",
    "    \n",
    "    fc = fully_conn(fc, num_outputs)        \n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    out = output(fc, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability})\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    test_valid_size = 256\n",
    "    \n",
    "    valid_loss = session.run(cost, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.0})\n",
    "    \n",
    "    train_loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.0})    \n",
    "    \n",
    "    train_acc = session.run(accuracy, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.0})\n",
    "\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.0})    \n",
    "\n",
    "    print('Validation Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(\n",
    "              valid_loss,valid_acc))\n",
    "    print('Train Loss: {:>10.4f} Train Accuracy: {:.6f}'.format(\n",
    "              train_loss,train_acc))\n",
    "            \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS TEST HISTORY \n",
    "---\n",
    "#### RUN 1\n",
    "epochs = 100 batch_size = 512 keep_probability = 0.75\n",
    "\n",
    "Epoch 100, CIFAR-10 Batch 1:  Validation Loss: 35216.7969 Validation Accuracy: 0.374400\n",
    "Train Loss: 22322.4883 Train Accuracy: 0.476351\n",
    "\n",
    "---\n",
    "#### RUN 2\n",
    "epochs = 128 batch_size = 512 keep_probability = 0.75\n",
    "\n",
    "Epoch 128, CIFAR-10 Batch 1:  Validation Loss: 19870.6484 Validation Accuracy: 0.337200\n",
    "Train Loss: 10584.4189 Train Accuracy: 0.472973\n",
    "\n",
    "---\n",
    "#### RUN 3\n",
    "epochs = 100 batch_size = 1024 keep_probability = 0.75\n",
    "\n",
    "Epoch 100, CIFAR-10 Batch 1:  Validation Loss: 84251.0312 Validation Accuracy: 0.393200\n",
    "Train Loss: 61735.8125 Train Accuracy: 0.455446\n",
    "\n",
    "---\n",
    "#### RUN 4\n",
    "epochs = 100 batch_size = 1024 keep_probability = 0.75\n",
    "\n",
    "Epoch 128, CIFAR-10 Batch 1:  Validation Loss: 77022.4531 Validation Accuracy: 0.374200\n",
    "Train Loss: 53328.5156 Train Accuracy: 0.455446"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Validation Loss:     2.2776 Validation Accuracy: 0.172000\n",
      "Train Loss:     2.2790 Train Accuracy: 0.178218\n",
      "Epoch  2, CIFAR-10 Batch 1:  Validation Loss:     2.0761 Validation Accuracy: 0.238000\n",
      "Train Loss:     2.1014 Train Accuracy: 0.232673\n",
      "Epoch  3, CIFAR-10 Batch 1:  Validation Loss:     2.0848 Validation Accuracy: 0.245200\n",
      "Train Loss:     2.1180 Train Accuracy: 0.221535\n",
      "Epoch  4, CIFAR-10 Batch 1:  Validation Loss:     2.0091 Validation Accuracy: 0.266400\n",
      "Train Loss:     2.0670 Train Accuracy: 0.250000\n",
      "Epoch  5, CIFAR-10 Batch 1:  Validation Loss:     1.9357 Validation Accuracy: 0.301200\n",
      "Train Loss:     1.9885 Train Accuracy: 0.295792\n",
      "Epoch  6, CIFAR-10 Batch 1:  Validation Loss:     1.8483 Validation Accuracy: 0.334400\n",
      "Train Loss:     1.8969 Train Accuracy: 0.326733\n",
      "Epoch  7, CIFAR-10 Batch 1:  Validation Loss:     1.7821 Validation Accuracy: 0.367200\n",
      "Train Loss:     1.8213 Train Accuracy: 0.367574\n",
      "Epoch  8, CIFAR-10 Batch 1:  Validation Loss:     1.7514 Validation Accuracy: 0.372000\n",
      "Train Loss:     1.7991 Train Accuracy: 0.371287\n",
      "Epoch  9, CIFAR-10 Batch 1:  Validation Loss:     1.6562 Validation Accuracy: 0.409400\n",
      "Train Loss:     1.6865 Train Accuracy: 0.409653\n",
      "Epoch 10, CIFAR-10 Batch 1:  Validation Loss:     1.6278 Validation Accuracy: 0.424000\n",
      "Train Loss:     1.6582 Train Accuracy: 0.417079\n",
      "Epoch 11, CIFAR-10 Batch 1:  Validation Loss:     1.5677 Validation Accuracy: 0.442800\n",
      "Train Loss:     1.5825 Train Accuracy: 0.449257\n",
      "Epoch 12, CIFAR-10 Batch 1:  Validation Loss:     1.5691 Validation Accuracy: 0.444000\n",
      "Train Loss:     1.5724 Train Accuracy: 0.449257\n",
      "Epoch 13, CIFAR-10 Batch 1:  Validation Loss:     1.5831 Validation Accuracy: 0.442200\n",
      "Train Loss:     1.5833 Train Accuracy: 0.441832\n",
      "Epoch 14, CIFAR-10 Batch 1:  Validation Loss:     1.4866 Validation Accuracy: 0.470600\n",
      "Train Loss:     1.4564 Train Accuracy: 0.475248\n",
      "Epoch 15, CIFAR-10 Batch 1:  Validation Loss:     1.5054 Validation Accuracy: 0.465200\n",
      "Train Loss:     1.4320 Train Accuracy: 0.496287\n",
      "Epoch 16, CIFAR-10 Batch 1:  Validation Loss:     1.4873 Validation Accuracy: 0.481400\n",
      "Train Loss:     1.4020 Train Accuracy: 0.533416\n",
      "Epoch 17, CIFAR-10 Batch 1:  Validation Loss:     1.4069 Validation Accuracy: 0.493600\n",
      "Train Loss:     1.3158 Train Accuracy: 0.539604\n",
      "Epoch 18, CIFAR-10 Batch 1:  Validation Loss:     1.4481 Validation Accuracy: 0.491800\n",
      "Train Loss:     1.3383 Train Accuracy: 0.530941\n",
      "Epoch 19, CIFAR-10 Batch 1:  Validation Loss:     1.4498 Validation Accuracy: 0.498200\n",
      "Train Loss:     1.3002 Train Accuracy: 0.547030\n",
      "Epoch 20, CIFAR-10 Batch 1:  Validation Loss:     1.3992 Validation Accuracy: 0.508600\n",
      "Train Loss:     1.1987 Train Accuracy: 0.577970\n",
      "Epoch 21, CIFAR-10 Batch 1:  Validation Loss:     1.4722 Validation Accuracy: 0.499800\n",
      "Train Loss:     1.2561 Train Accuracy: 0.574257\n",
      "Epoch 22, CIFAR-10 Batch 1:  Validation Loss:     1.3668 Validation Accuracy: 0.520200\n",
      "Train Loss:     1.1427 Train Accuracy: 0.591584\n",
      "Epoch 23, CIFAR-10 Batch 1:  Validation Loss:     1.3374 Validation Accuracy: 0.535200\n",
      "Train Loss:     1.0733 Train Accuracy: 0.618812\n",
      "Epoch 24, CIFAR-10 Batch 1:  Validation Loss:     1.3659 Validation Accuracy: 0.531400\n",
      "Train Loss:     1.0756 Train Accuracy: 0.617574\n",
      "Epoch 25, CIFAR-10 Batch 1:  Validation Loss:     1.2610 Validation Accuracy: 0.552800\n",
      "Train Loss:     0.9358 Train Accuracy: 0.681931\n",
      "Epoch 26, CIFAR-10 Batch 1:  Validation Loss:     1.2400 Validation Accuracy: 0.561600\n",
      "Train Loss:     0.8580 Train Accuracy: 0.700495\n",
      "Epoch 27, CIFAR-10 Batch 1:  Validation Loss:     1.2852 Validation Accuracy: 0.555000\n",
      "Train Loss:     0.8847 Train Accuracy: 0.688119\n",
      "Epoch 28, CIFAR-10 Batch 1:  Validation Loss:     1.2709 Validation Accuracy: 0.564600\n",
      "Train Loss:     0.8137 Train Accuracy: 0.720297\n",
      "Epoch 29, CIFAR-10 Batch 1:  Validation Loss:     1.2652 Validation Accuracy: 0.564800\n",
      "Train Loss:     0.7445 Train Accuracy: 0.763614\n",
      "Epoch 30, CIFAR-10 Batch 1:  Validation Loss:     1.2747 Validation Accuracy: 0.556600\n",
      "Train Loss:     0.7254 Train Accuracy: 0.764851\n",
      "Epoch 31, CIFAR-10 Batch 1:  Validation Loss:     1.2863 Validation Accuracy: 0.562200\n",
      "Train Loss:     0.6663 Train Accuracy: 0.778465\n",
      "Epoch 32, CIFAR-10 Batch 1:  Validation Loss:     1.2792 Validation Accuracy: 0.564600\n",
      "Train Loss:     0.5985 Train Accuracy: 0.820545\n",
      "Epoch 33, CIFAR-10 Batch 1:  Validation Loss:     1.3118 Validation Accuracy: 0.562400\n",
      "Train Loss:     0.6224 Train Accuracy: 0.793317\n",
      "Epoch 34, CIFAR-10 Batch 1:  Validation Loss:     1.2945 Validation Accuracy: 0.568600\n",
      "Train Loss:     0.5356 Train Accuracy: 0.846535\n",
      "Epoch 35, CIFAR-10 Batch 1:  Validation Loss:     1.3154 Validation Accuracy: 0.577600\n",
      "Train Loss:     0.5088 Train Accuracy: 0.841584\n",
      "Epoch 36, CIFAR-10 Batch 1:  Validation Loss:     1.3184 Validation Accuracy: 0.567800\n",
      "Train Loss:     0.4540 Train Accuracy: 0.863861\n",
      "Epoch 37, CIFAR-10 Batch 1:  Validation Loss:     1.3246 Validation Accuracy: 0.575400\n",
      "Train Loss:     0.4297 Train Accuracy: 0.870049\n",
      "Epoch 38, CIFAR-10 Batch 1:  Validation Loss:     1.4050 Validation Accuracy: 0.552200\n",
      "Train Loss:     0.5675 Train Accuracy: 0.814356\n",
      "Epoch 39, CIFAR-10 Batch 1:  Validation Loss:     1.4033 Validation Accuracy: 0.557400\n",
      "Train Loss:     0.4523 Train Accuracy: 0.860148\n",
      "Epoch 40, CIFAR-10 Batch 1:  Validation Loss:     1.3440 Validation Accuracy: 0.578600\n",
      "Train Loss:     0.3475 Train Accuracy: 0.904703\n",
      "Epoch 41, CIFAR-10 Batch 1:  Validation Loss:     1.4093 Validation Accuracy: 0.570000\n",
      "Train Loss:     0.3464 Train Accuracy: 0.910891\n",
      "Epoch 42, CIFAR-10 Batch 1:  Validation Loss:     1.5405 Validation Accuracy: 0.544000\n",
      "Train Loss:     0.3970 Train Accuracy: 0.870049\n",
      "Epoch 43, CIFAR-10 Batch 1:  Validation Loss:     1.5620 Validation Accuracy: 0.546400\n",
      "Train Loss:     0.3432 Train Accuracy: 0.897277\n",
      "Epoch 44, CIFAR-10 Batch 1:  Validation Loss:     1.5223 Validation Accuracy: 0.566600\n",
      "Train Loss:     0.3004 Train Accuracy: 0.908416\n",
      "Epoch 45, CIFAR-10 Batch 1:  Validation Loss:     1.6008 Validation Accuracy: 0.559600\n",
      "Train Loss:     0.2560 Train Accuracy: 0.918317\n",
      "Epoch 46, CIFAR-10 Batch 1:  Validation Loss:     1.5486 Validation Accuracy: 0.577000\n",
      "Train Loss:     0.1887 Train Accuracy: 0.950495\n",
      "Epoch 47, CIFAR-10 Batch 1:  Validation Loss:     1.5711 Validation Accuracy: 0.566800\n",
      "Train Loss:     0.1608 Train Accuracy: 0.965347\n",
      "Epoch 48, CIFAR-10 Batch 1:  Validation Loss:     1.6297 Validation Accuracy: 0.562600\n",
      "Train Loss:     0.1614 Train Accuracy: 0.967822\n",
      "Epoch 49, CIFAR-10 Batch 1:  Validation Loss:     1.6803 Validation Accuracy: 0.571400\n",
      "Train Loss:     0.1308 Train Accuracy: 0.976485\n",
      "Epoch 50, CIFAR-10 Batch 1:  Validation Loss:     1.7835 Validation Accuracy: 0.560600\n",
      "Train Loss:     0.1439 Train Accuracy: 0.961634\n",
      "Epoch 51, CIFAR-10 Batch 1:  Validation Loss:     1.7786 Validation Accuracy: 0.556000\n",
      "Train Loss:     0.1377 Train Accuracy: 0.971535\n",
      "Epoch 52, CIFAR-10 Batch 1:  Validation Loss:     1.7293 Validation Accuracy: 0.565600\n",
      "Train Loss:     0.1338 Train Accuracy: 0.970297\n",
      "Epoch 53, CIFAR-10 Batch 1:  Validation Loss:     1.6605 Validation Accuracy: 0.564600\n",
      "Train Loss:     0.1184 Train Accuracy: 0.977723\n",
      "Epoch 54, CIFAR-10 Batch 1:  Validation Loss:     1.6783 Validation Accuracy: 0.569200\n",
      "Train Loss:     0.1336 Train Accuracy: 0.969059\n",
      "Epoch 55, CIFAR-10 Batch 1:  Validation Loss:     1.7922 Validation Accuracy: 0.555600\n",
      "Train Loss:     0.1425 Train Accuracy: 0.964109\n",
      "Epoch 56, CIFAR-10 Batch 1:  Validation Loss:     1.8356 Validation Accuracy: 0.558400\n",
      "Train Loss:     0.0998 Train Accuracy: 0.985148\n",
      "Epoch 57, CIFAR-10 Batch 1:  Validation Loss:     1.8482 Validation Accuracy: 0.556200\n",
      "Train Loss:     0.0874 Train Accuracy: 0.980198\n",
      "Epoch 58, CIFAR-10 Batch 1:  Validation Loss:     1.8115 Validation Accuracy: 0.572000\n",
      "Train Loss:     0.0576 Train Accuracy: 0.995049\n",
      "Epoch 59, CIFAR-10 Batch 1:  Validation Loss:     1.9413 Validation Accuracy: 0.568000\n",
      "Train Loss:     0.0647 Train Accuracy: 0.986386\n",
      "Epoch 60, CIFAR-10 Batch 1:  Validation Loss:     1.9999 Validation Accuracy: 0.565800\n",
      "Train Loss:     0.0636 Train Accuracy: 0.981436\n",
      "Epoch 61, CIFAR-10 Batch 1:  Validation Loss:     2.0437 Validation Accuracy: 0.559000\n",
      "Train Loss:     0.0602 Train Accuracy: 0.991337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, CIFAR-10 Batch 1:  Validation Loss:     2.0554 Validation Accuracy: 0.560000\n",
      "Train Loss:     0.0508 Train Accuracy: 0.992574\n",
      "Epoch 63, CIFAR-10 Batch 1:  Validation Loss:     1.9446 Validation Accuracy: 0.573400\n",
      "Train Loss:     0.0467 Train Accuracy: 0.993812\n",
      "Epoch 64, CIFAR-10 Batch 1:  Validation Loss:     1.9308 Validation Accuracy: 0.575600\n",
      "Train Loss:     0.0560 Train Accuracy: 0.995049\n",
      "Epoch 65, CIFAR-10 Batch 1:  Validation Loss:     2.0504 Validation Accuracy: 0.558400\n",
      "Train Loss:     0.0881 Train Accuracy: 0.971535\n",
      "Epoch 66, CIFAR-10 Batch 1:  Validation Loss:     2.1275 Validation Accuracy: 0.566600\n",
      "Train Loss:     0.0769 Train Accuracy: 0.981436\n",
      "Epoch 67, CIFAR-10 Batch 1:  Validation Loss:     2.1509 Validation Accuracy: 0.561400\n",
      "Train Loss:     0.0530 Train Accuracy: 0.992574\n",
      "Epoch 68, CIFAR-10 Batch 1:  Validation Loss:     2.2549 Validation Accuracy: 0.558400\n",
      "Train Loss:     0.0630 Train Accuracy: 0.988861\n",
      "Epoch 69, CIFAR-10 Batch 1:  Validation Loss:     2.0749 Validation Accuracy: 0.575200\n",
      "Train Loss:     0.0444 Train Accuracy: 0.992574\n",
      "Epoch 70, CIFAR-10 Batch 1:  Validation Loss:     2.0745 Validation Accuracy: 0.574400\n",
      "Train Loss:     0.0404 Train Accuracy: 0.993812\n",
      "Epoch 71, CIFAR-10 Batch 1:  Validation Loss:     2.1075 Validation Accuracy: 0.578600\n",
      "Train Loss:     0.0346 Train Accuracy: 0.995049\n",
      "Epoch 72, CIFAR-10 Batch 1:  Validation Loss:     2.0197 Validation Accuracy: 0.575800\n",
      "Train Loss:     0.0341 Train Accuracy: 0.995049\n",
      "Epoch 73, CIFAR-10 Batch 1:  Validation Loss:     2.2085 Validation Accuracy: 0.571400\n",
      "Train Loss:     0.0449 Train Accuracy: 0.990099\n",
      "Epoch 74, CIFAR-10 Batch 1:  Validation Loss:     2.1938 Validation Accuracy: 0.569800\n",
      "Train Loss:     0.0413 Train Accuracy: 0.991337\n",
      "Epoch 75, CIFAR-10 Batch 1:  Validation Loss:     2.1602 Validation Accuracy: 0.576400\n",
      "Train Loss:     0.0370 Train Accuracy: 0.996287\n",
      "Epoch 76, CIFAR-10 Batch 1:  Validation Loss:     2.2940 Validation Accuracy: 0.561600\n",
      "Train Loss:     0.0548 Train Accuracy: 0.982673\n",
      "Epoch 77, CIFAR-10 Batch 1:  Validation Loss:     2.2152 Validation Accuracy: 0.574000\n",
      "Train Loss:     0.0634 Train Accuracy: 0.977723\n",
      "Epoch 78, CIFAR-10 Batch 1:  Validation Loss:     2.0596 Validation Accuracy: 0.564200\n",
      "Train Loss:     0.0268 Train Accuracy: 0.998762\n",
      "Epoch 79, CIFAR-10 Batch 1:  Validation Loss:     2.0644 Validation Accuracy: 0.564800\n",
      "Train Loss:     0.0426 Train Accuracy: 0.992574\n",
      "Epoch 80, CIFAR-10 Batch 1:  Validation Loss:     2.3738 Validation Accuracy: 0.538800\n",
      "Train Loss:     0.1077 Train Accuracy: 0.959158\n",
      "Epoch 81, CIFAR-10 Batch 1:  Validation Loss:     2.1992 Validation Accuracy: 0.556400\n",
      "Train Loss:     0.0346 Train Accuracy: 0.992574\n",
      "Epoch 82, CIFAR-10 Batch 1:  Validation Loss:     2.2369 Validation Accuracy: 0.566200\n",
      "Train Loss:     0.0252 Train Accuracy: 0.998762\n",
      "Epoch 83, CIFAR-10 Batch 1:  Validation Loss:     2.2618 Validation Accuracy: 0.576000\n",
      "Train Loss:     0.0281 Train Accuracy: 0.995049\n",
      "Epoch 84, CIFAR-10 Batch 1:  Validation Loss:     2.3359 Validation Accuracy: 0.579200\n",
      "Train Loss:     0.0176 Train Accuracy: 0.996287\n",
      "Epoch 85, CIFAR-10 Batch 1:  Validation Loss:     2.4040 Validation Accuracy: 0.566200\n",
      "Train Loss:     0.0157 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Validation Loss:     2.5537 Validation Accuracy: 0.564000\n",
      "Train Loss:     0.0103 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Validation Loss:     2.5160 Validation Accuracy: 0.573400\n",
      "Train Loss:     0.0271 Train Accuracy: 0.990099\n",
      "Epoch 88, CIFAR-10 Batch 1:  Validation Loss:     2.4828 Validation Accuracy: 0.575000\n",
      "Train Loss:     0.0201 Train Accuracy: 0.996287\n",
      "Epoch 89, CIFAR-10 Batch 1:  Validation Loss:     2.5483 Validation Accuracy: 0.574400\n",
      "Train Loss:     0.0070 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Validation Loss:     2.4240 Validation Accuracy: 0.580000\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Validation Loss:     2.4956 Validation Accuracy: 0.571000\n",
      "Train Loss:     0.0069 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  Validation Loss:     2.4542 Validation Accuracy: 0.574000\n",
      "Train Loss:     0.0055 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Validation Loss:     2.4858 Validation Accuracy: 0.571000\n",
      "Train Loss:     0.0069 Train Accuracy: 0.998762\n",
      "Epoch 94, CIFAR-10 Batch 1:  Validation Loss:     2.5410 Validation Accuracy: 0.564800\n",
      "Train Loss:     0.0049 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Validation Loss:     2.5892 Validation Accuracy: 0.559400\n",
      "Train Loss:     0.0069 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Validation Loss:     2.6556 Validation Accuracy: 0.554600\n",
      "Train Loss:     0.0099 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Validation Loss:     2.6480 Validation Accuracy: 0.551600\n",
      "Train Loss:     0.0117 Train Accuracy: 0.997525\n",
      "Epoch 98, CIFAR-10 Batch 1:  Validation Loss:     2.5511 Validation Accuracy: 0.554400\n",
      "Train Loss:     0.0106 Train Accuracy: 0.998762\n",
      "Epoch 99, CIFAR-10 Batch 1:  Validation Loss:     2.4367 Validation Accuracy: 0.552400\n",
      "Train Loss:     0.0074 Train Accuracy: 0.998762\n",
      "Epoch 100, CIFAR-10 Batch 1:  Validation Loss:     2.4099 Validation Accuracy: 0.565600\n",
      "Train Loss:     0.0176 Train Accuracy: 0.997525\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Validation Loss:     2.2763 Validation Accuracy: 0.140600\n",
      "Train Loss:     2.2753 Train Accuracy: 0.152228\n",
      "Epoch  1, CIFAR-10 Batch 2:  Validation Loss:     2.2141 Validation Accuracy: 0.174000\n",
      "Train Loss:     2.2157 Train Accuracy: 0.164604\n",
      "Epoch  1, CIFAR-10 Batch 3:  Validation Loss:     2.2561 Validation Accuracy: 0.161800\n",
      "Train Loss:     2.2303 Train Accuracy: 0.165842\n",
      "Epoch  1, CIFAR-10 Batch 4:  Validation Loss:     2.0969 Validation Accuracy: 0.234600\n",
      "Train Loss:     2.0932 Train Accuracy: 0.228960\n",
      "Epoch  1, CIFAR-10 Batch 5:  Validation Loss:     2.2429 Validation Accuracy: 0.210000\n",
      "Train Loss:     2.2260 Train Accuracy: 0.212871\n",
      "Epoch  2, CIFAR-10 Batch 1:  Validation Loss:     1.9189 Validation Accuracy: 0.308600\n",
      "Train Loss:     1.9941 Train Accuracy: 0.300743\n",
      "Epoch  2, CIFAR-10 Batch 2:  Validation Loss:     1.8979 Validation Accuracy: 0.314000\n",
      "Train Loss:     1.8866 Train Accuracy: 0.330446\n",
      "Epoch  2, CIFAR-10 Batch 3:  Validation Loss:     1.9428 Validation Accuracy: 0.310800\n",
      "Train Loss:     1.9138 Train Accuracy: 0.337871\n",
      "Epoch  2, CIFAR-10 Batch 4:  Validation Loss:     1.9386 Validation Accuracy: 0.313200\n",
      "Train Loss:     1.9375 Train Accuracy: 0.337871\n",
      "Epoch  2, CIFAR-10 Batch 5:  Validation Loss:     1.9237 Validation Accuracy: 0.346400\n",
      "Train Loss:     1.8867 Train Accuracy: 0.346535\n",
      "Epoch  3, CIFAR-10 Batch 1:  Validation Loss:     1.6422 Validation Accuracy: 0.415400\n",
      "Train Loss:     1.6911 Train Accuracy: 0.414604\n",
      "Epoch  3, CIFAR-10 Batch 2:  Validation Loss:     1.8029 Validation Accuracy: 0.362200\n",
      "Train Loss:     1.7746 Train Accuracy: 0.399752\n",
      "Epoch  3, CIFAR-10 Batch 3:  Validation Loss:     1.7111 Validation Accuracy: 0.396400\n",
      "Train Loss:     1.6724 Train Accuracy: 0.424505\n",
      "Epoch  3, CIFAR-10 Batch 4:  Validation Loss:     1.7428 Validation Accuracy: 0.395600\n",
      "Train Loss:     1.7210 Train Accuracy: 0.424505\n",
      "Epoch  3, CIFAR-10 Batch 5:  Validation Loss:     1.6489 Validation Accuracy: 0.429200\n",
      "Train Loss:     1.5884 Train Accuracy: 0.460396\n",
      "Epoch  4, CIFAR-10 Batch 1:  Validation Loss:     1.7470 Validation Accuracy: 0.413600\n",
      "Train Loss:     1.8054 Train Accuracy: 0.407178\n",
      "Epoch  4, CIFAR-10 Batch 2:  Validation Loss:     1.5823 Validation Accuracy: 0.447000\n",
      "Train Loss:     1.5142 Train Accuracy: 0.471535\n",
      "Epoch  4, CIFAR-10 Batch 3:  Validation Loss:     1.8936 Validation Accuracy: 0.376400\n",
      "Train Loss:     1.7802 Train Accuracy: 0.420792\n",
      "Epoch  4, CIFAR-10 Batch 4:  Validation Loss:     1.4675 Validation Accuracy: 0.477800\n",
      "Train Loss:     1.4120 Train Accuracy: 0.514851\n",
      "Epoch  4, CIFAR-10 Batch 5:  Validation Loss:     1.8236 Validation Accuracy: 0.419200\n",
      "Train Loss:     1.7411 Train Accuracy: 0.455446\n",
      "Epoch  5, CIFAR-10 Batch 1:  Validation Loss:     1.8875 Validation Accuracy: 0.398600\n",
      "Train Loss:     1.9595 Train Accuracy: 0.387376\n",
      "Epoch  5, CIFAR-10 Batch 2:  Validation Loss:     1.6922 Validation Accuracy: 0.424800\n",
      "Train Loss:     1.6646 Train Accuracy: 0.454208\n",
      "Epoch  5, CIFAR-10 Batch 3:  Validation Loss:     1.4415 Validation Accuracy: 0.503000\n",
      "Train Loss:     1.3479 Train Accuracy: 0.530941\n",
      "Epoch  5, CIFAR-10 Batch 4:  Validation Loss:     1.3681 Validation Accuracy: 0.523200\n",
      "Train Loss:     1.2869 Train Accuracy: 0.553218\n",
      "Epoch  5, CIFAR-10 Batch 5:  Validation Loss:     1.4791 Validation Accuracy: 0.497400\n",
      "Train Loss:     1.3796 Train Accuracy: 0.537129\n",
      "Epoch  6, CIFAR-10 Batch 1:  Validation Loss:     1.5907 Validation Accuracy: 0.482000\n",
      "Train Loss:     1.6136 Train Accuracy: 0.470297\n",
      "Epoch  6, CIFAR-10 Batch 2:  Validation Loss:     1.5832 Validation Accuracy: 0.462400\n",
      "Train Loss:     1.5238 Train Accuracy: 0.486386\n",
      "Epoch  6, CIFAR-10 Batch 3:  Validation Loss:     1.4053 Validation Accuracy: 0.515400\n",
      "Train Loss:     1.2785 Train Accuracy: 0.561881\n",
      "Epoch  6, CIFAR-10 Batch 4:  Validation Loss:     1.5120 Validation Accuracy: 0.484800\n",
      "Train Loss:     1.3969 Train Accuracy: 0.537129\n",
      "Epoch  6, CIFAR-10 Batch 5:  Validation Loss:     1.3893 Validation Accuracy: 0.518400\n",
      "Train Loss:     1.2748 Train Accuracy: 0.561881\n",
      "Epoch  7, CIFAR-10 Batch 1:  Validation Loss:     1.4352 Validation Accuracy: 0.517200\n",
      "Train Loss:     1.4401 Train Accuracy: 0.519802\n",
      "Epoch  7, CIFAR-10 Batch 2:  Validation Loss:     1.3924 Validation Accuracy: 0.530200\n",
      "Train Loss:     1.3105 Train Accuracy: 0.548267\n",
      "Epoch  7, CIFAR-10 Batch 3:  Validation Loss:     1.3014 Validation Accuracy: 0.548600\n",
      "Train Loss:     1.1539 Train Accuracy: 0.592822\n",
      "Epoch  7, CIFAR-10 Batch 4:  Validation Loss:     1.2960 Validation Accuracy: 0.548200\n",
      "Train Loss:     1.1798 Train Accuracy: 0.592822\n",
      "Epoch  7, CIFAR-10 Batch 5:  Validation Loss:     1.3310 Validation Accuracy: 0.539600\n",
      "Train Loss:     1.1881 Train Accuracy: 0.607673\n",
      "Epoch  8, CIFAR-10 Batch 1:  Validation Loss:     1.2675 Validation Accuracy: 0.573200\n",
      "Train Loss:     1.2330 Train Accuracy: 0.569307\n",
      "Epoch  8, CIFAR-10 Batch 2:  Validation Loss:     1.2363 Validation Accuracy: 0.575200\n",
      "Train Loss:     1.1369 Train Accuracy: 0.615099\n",
      "Epoch  8, CIFAR-10 Batch 3:  Validation Loss:     1.1730 Validation Accuracy: 0.590800\n",
      "Train Loss:     1.0141 Train Accuracy: 0.638614\n",
      "Epoch  8, CIFAR-10 Batch 4:  Validation Loss:     1.3108 Validation Accuracy: 0.546400\n",
      "Train Loss:     1.1728 Train Accuracy: 0.585396\n",
      "Epoch  8, CIFAR-10 Batch 5:  Validation Loss:     1.2714 Validation Accuracy: 0.555600\n",
      "Train Loss:     1.1069 Train Accuracy: 0.613861\n",
      "Epoch  9, CIFAR-10 Batch 1:  Validation Loss:     1.1347 Validation Accuracy: 0.613400\n",
      "Train Loss:     1.0593 Train Accuracy: 0.622525\n",
      "Epoch  9, CIFAR-10 Batch 2:  Validation Loss:     1.1699 Validation Accuracy: 0.595400\n",
      "Train Loss:     1.0396 Train Accuracy: 0.643564\n",
      "Epoch  9, CIFAR-10 Batch 3:  Validation Loss:     1.1102 Validation Accuracy: 0.617000\n",
      "Train Loss:     0.9278 Train Accuracy: 0.655941\n",
      "Epoch  9, CIFAR-10 Batch 4:  Validation Loss:     1.1007 Validation Accuracy: 0.610200\n",
      "Train Loss:     0.9392 Train Accuracy: 0.664604\n",
      "Epoch  9, CIFAR-10 Batch 5:  Validation Loss:     1.1016 Validation Accuracy: 0.615000\n",
      "Train Loss:     0.9139 Train Accuracy: 0.681931\n",
      "Epoch 10, CIFAR-10 Batch 1:  Validation Loss:     1.0705 Validation Accuracy: 0.637200\n",
      "Train Loss:     0.9601 Train Accuracy: 0.663366\n",
      "Epoch 10, CIFAR-10 Batch 2:  Validation Loss:     1.0451 Validation Accuracy: 0.630800\n",
      "Train Loss:     0.8840 Train Accuracy: 0.701733\n",
      "Epoch 10, CIFAR-10 Batch 3:  Validation Loss:     1.0483 Validation Accuracy: 0.634400\n",
      "Train Loss:     0.8336 Train Accuracy: 0.693069\n",
      "Epoch 10, CIFAR-10 Batch 4:  Validation Loss:     1.0621 Validation Accuracy: 0.627800\n",
      "Train Loss:     0.8797 Train Accuracy: 0.686881\n",
      "Epoch 10, CIFAR-10 Batch 5:  Validation Loss:     1.1061 Validation Accuracy: 0.617800\n",
      "Train Loss:     0.8892 Train Accuracy: 0.695545\n",
      "Epoch 11, CIFAR-10 Batch 1:  Validation Loss:     1.0030 Validation Accuracy: 0.659800\n",
      "Train Loss:     0.8514 Train Accuracy: 0.704208\n",
      "Epoch 11, CIFAR-10 Batch 2:  Validation Loss:     0.9752 Validation Accuracy: 0.658200\n",
      "Train Loss:     0.7685 Train Accuracy: 0.743812\n",
      "Epoch 11, CIFAR-10 Batch 3:  Validation Loss:     1.0550 Validation Accuracy: 0.639000\n",
      "Train Loss:     0.8123 Train Accuracy: 0.704208\n",
      "Epoch 11, CIFAR-10 Batch 4:  Validation Loss:     1.0640 Validation Accuracy: 0.633000\n",
      "Train Loss:     0.8521 Train Accuracy: 0.711634\n",
      "Epoch 11, CIFAR-10 Batch 5:  Validation Loss:     1.0316 Validation Accuracy: 0.646400\n",
      "Train Loss:     0.7754 Train Accuracy: 0.732673\n",
      "Epoch 12, CIFAR-10 Batch 1:  Validation Loss:     1.0419 Validation Accuracy: 0.644800\n",
      "Train Loss:     0.8509 Train Accuracy: 0.701733\n",
      "Epoch 12, CIFAR-10 Batch 2:  Validation Loss:     1.0701 Validation Accuracy: 0.633400\n",
      "Train Loss:     0.8519 Train Accuracy: 0.717822\n",
      "Epoch 12, CIFAR-10 Batch 3:  Validation Loss:     1.0707 Validation Accuracy: 0.637600\n",
      "Train Loss:     0.7741 Train Accuracy: 0.724010\n",
      "Epoch 12, CIFAR-10 Batch 4:  Validation Loss:     1.0671 Validation Accuracy: 0.633400\n",
      "Train Loss:     0.8126 Train Accuracy: 0.726485\n",
      "Epoch 12, CIFAR-10 Batch 5:  Validation Loss:     0.9732 Validation Accuracy: 0.669400\n",
      "Train Loss:     0.6780 Train Accuracy: 0.769802\n",
      "Epoch 13, CIFAR-10 Batch 1:  Validation Loss:     0.9531 Validation Accuracy: 0.674800\n",
      "Train Loss:     0.7068 Train Accuracy: 0.759901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, CIFAR-10 Batch 2:  Validation Loss:     1.0056 Validation Accuracy: 0.655800\n",
      "Train Loss:     0.7118 Train Accuracy: 0.754951\n",
      "Epoch 13, CIFAR-10 Batch 3:  Validation Loss:     1.0896 Validation Accuracy: 0.636400\n",
      "Train Loss:     0.7282 Train Accuracy: 0.740099\n",
      "Epoch 13, CIFAR-10 Batch 4:  Validation Loss:     0.9685 Validation Accuracy: 0.666600\n",
      "Train Loss:     0.6883 Train Accuracy: 0.757426\n",
      "Epoch 13, CIFAR-10 Batch 5:  Validation Loss:     0.9521 Validation Accuracy: 0.671800\n",
      "Train Loss:     0.6290 Train Accuracy: 0.793317\n",
      "Epoch 14, CIFAR-10 Batch 1:  Validation Loss:     0.9790 Validation Accuracy: 0.672600\n",
      "Train Loss:     0.6940 Train Accuracy: 0.758663\n",
      "Epoch 14, CIFAR-10 Batch 2:  Validation Loss:     1.0694 Validation Accuracy: 0.639800\n",
      "Train Loss:     0.7501 Train Accuracy: 0.740099\n",
      "Epoch 14, CIFAR-10 Batch 3:  Validation Loss:     1.1131 Validation Accuracy: 0.635400\n",
      "Train Loss:     0.7122 Train Accuracy: 0.747525\n",
      "Epoch 14, CIFAR-10 Batch 4:  Validation Loss:     0.9904 Validation Accuracy: 0.658200\n",
      "Train Loss:     0.6400 Train Accuracy: 0.774752\n",
      "Epoch 14, CIFAR-10 Batch 5:  Validation Loss:     0.9371 Validation Accuracy: 0.676400\n",
      "Train Loss:     0.5778 Train Accuracy: 0.798267\n",
      "Epoch 15, CIFAR-10 Batch 1:  Validation Loss:     0.9875 Validation Accuracy: 0.668200\n",
      "Train Loss:     0.6903 Train Accuracy: 0.754950\n",
      "Epoch 15, CIFAR-10 Batch 2:  Validation Loss:     1.0839 Validation Accuracy: 0.641600\n",
      "Train Loss:     0.7406 Train Accuracy: 0.757426\n",
      "Epoch 15, CIFAR-10 Batch 3:  Validation Loss:     1.0379 Validation Accuracy: 0.655600\n",
      "Train Loss:     0.6060 Train Accuracy: 0.782178\n",
      "Epoch 15, CIFAR-10 Batch 4:  Validation Loss:     0.9726 Validation Accuracy: 0.672600\n",
      "Train Loss:     0.5985 Train Accuracy: 0.783416\n",
      "Epoch 15, CIFAR-10 Batch 5:  Validation Loss:     0.9771 Validation Accuracy: 0.669200\n",
      "Train Loss:     0.5713 Train Accuracy: 0.793317\n",
      "Epoch 16, CIFAR-10 Batch 1:  Validation Loss:     0.9895 Validation Accuracy: 0.675200\n",
      "Train Loss:     0.6584 Train Accuracy: 0.772277\n",
      "Epoch 16, CIFAR-10 Batch 2:  Validation Loss:     1.0780 Validation Accuracy: 0.645000\n",
      "Train Loss:     0.6896 Train Accuracy: 0.772277\n",
      "Epoch 16, CIFAR-10 Batch 3:  Validation Loss:     1.1075 Validation Accuracy: 0.646800\n",
      "Train Loss:     0.6440 Train Accuracy: 0.767327\n",
      "Epoch 16, CIFAR-10 Batch 4:  Validation Loss:     0.9629 Validation Accuracy: 0.666000\n",
      "Train Loss:     0.5445 Train Accuracy: 0.808168\n",
      "Epoch 16, CIFAR-10 Batch 5:  Validation Loss:     0.9663 Validation Accuracy: 0.674800\n",
      "Train Loss:     0.5118 Train Accuracy: 0.821782\n",
      "Epoch 17, CIFAR-10 Batch 1:  Validation Loss:     1.0446 Validation Accuracy: 0.669800\n",
      "Train Loss:     0.6750 Train Accuracy: 0.758663\n",
      "Epoch 17, CIFAR-10 Batch 2:  Validation Loss:     1.0647 Validation Accuracy: 0.658000\n",
      "Train Loss:     0.6621 Train Accuracy: 0.778465\n",
      "Epoch 17, CIFAR-10 Batch 3:  Validation Loss:     1.0144 Validation Accuracy: 0.668800\n",
      "Train Loss:     0.5250 Train Accuracy: 0.813119\n",
      "Epoch 17, CIFAR-10 Batch 4:  Validation Loss:     0.9164 Validation Accuracy: 0.679800\n",
      "Train Loss:     0.4750 Train Accuracy: 0.847772\n",
      "Epoch 17, CIFAR-10 Batch 5:  Validation Loss:     0.9416 Validation Accuracy: 0.693600\n",
      "Train Loss:     0.4420 Train Accuracy: 0.849010\n",
      "Epoch 18, CIFAR-10 Batch 1:  Validation Loss:     0.9840 Validation Accuracy: 0.685600\n",
      "Train Loss:     0.5490 Train Accuracy: 0.805693\n",
      "Epoch 18, CIFAR-10 Batch 2:  Validation Loss:     1.0690 Validation Accuracy: 0.657800\n",
      "Train Loss:     0.6439 Train Accuracy: 0.785891\n",
      "Epoch 18, CIFAR-10 Batch 3:  Validation Loss:     0.9705 Validation Accuracy: 0.682000\n",
      "Train Loss:     0.4409 Train Accuracy: 0.841584\n",
      "Epoch 18, CIFAR-10 Batch 4:  Validation Loss:     0.8941 Validation Accuracy: 0.702400\n",
      "Train Loss:     0.4094 Train Accuracy: 0.860148\n",
      "Epoch 18, CIFAR-10 Batch 5:  Validation Loss:     1.0289 Validation Accuracy: 0.671600\n",
      "Train Loss:     0.4763 Train Accuracy: 0.829208\n",
      "Epoch 19, CIFAR-10 Batch 1:  Validation Loss:     0.9501 Validation Accuracy: 0.691200\n",
      "Train Loss:     0.5130 Train Accuracy: 0.818069\n",
      "Epoch 19, CIFAR-10 Batch 2:  Validation Loss:     0.8831 Validation Accuracy: 0.701600\n",
      "Train Loss:     0.4599 Train Accuracy: 0.851485\n",
      "Epoch 19, CIFAR-10 Batch 3:  Validation Loss:     0.9266 Validation Accuracy: 0.695200\n",
      "Train Loss:     0.3611 Train Accuracy: 0.867574\n",
      "Epoch 19, CIFAR-10 Batch 4:  Validation Loss:     0.8859 Validation Accuracy: 0.708800\n",
      "Train Loss:     0.3801 Train Accuracy: 0.873762\n",
      "Epoch 19, CIFAR-10 Batch 5:  Validation Loss:     0.9486 Validation Accuracy: 0.685800\n",
      "Train Loss:     0.3945 Train Accuracy: 0.863861\n",
      "Epoch 20, CIFAR-10 Batch 1:  Validation Loss:     0.8780 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.4017 Train Accuracy: 0.875000\n",
      "Epoch 20, CIFAR-10 Batch 2:  Validation Loss:     0.8981 Validation Accuracy: 0.697400\n",
      "Train Loss:     0.4177 Train Accuracy: 0.853960\n",
      "Epoch 20, CIFAR-10 Batch 3:  Validation Loss:     0.9543 Validation Accuracy: 0.694600\n",
      "Train Loss:     0.3587 Train Accuracy: 0.878713\n",
      "Epoch 20, CIFAR-10 Batch 4:  Validation Loss:     0.9832 Validation Accuracy: 0.690000\n",
      "Train Loss:     0.4089 Train Accuracy: 0.858911\n",
      "Epoch 20, CIFAR-10 Batch 5:  Validation Loss:     1.0158 Validation Accuracy: 0.672000\n",
      "Train Loss:     0.4220 Train Accuracy: 0.846535\n",
      "Epoch 21, CIFAR-10 Batch 1:  Validation Loss:     0.8548 Validation Accuracy: 0.714000\n",
      "Train Loss:     0.3553 Train Accuracy: 0.881188\n",
      "Epoch 21, CIFAR-10 Batch 2:  Validation Loss:     0.9935 Validation Accuracy: 0.685400\n",
      "Train Loss:     0.4621 Train Accuracy: 0.844059\n",
      "Epoch 21, CIFAR-10 Batch 3:  Validation Loss:     0.9213 Validation Accuracy: 0.704400\n",
      "Train Loss:     0.3034 Train Accuracy: 0.907178\n",
      "Epoch 21, CIFAR-10 Batch 4:  Validation Loss:     0.9140 Validation Accuracy: 0.706000\n",
      "Train Loss:     0.2977 Train Accuracy: 0.905941\n",
      "Epoch 21, CIFAR-10 Batch 5:  Validation Loss:     0.8837 Validation Accuracy: 0.704600\n",
      "Train Loss:     0.2918 Train Accuracy: 0.912129\n",
      "Epoch 22, CIFAR-10 Batch 1:  Validation Loss:     0.9296 Validation Accuracy: 0.699600\n",
      "Train Loss:     0.3688 Train Accuracy: 0.875000\n",
      "Epoch 22, CIFAR-10 Batch 2:  Validation Loss:     0.9660 Validation Accuracy: 0.683800\n",
      "Train Loss:     0.4010 Train Accuracy: 0.868812\n",
      "Epoch 22, CIFAR-10 Batch 3:  Validation Loss:     0.9092 Validation Accuracy: 0.706400\n",
      "Train Loss:     0.2889 Train Accuracy: 0.902228\n",
      "Epoch 22, CIFAR-10 Batch 4:  Validation Loss:     0.9257 Validation Accuracy: 0.701000\n",
      "Train Loss:     0.2741 Train Accuracy: 0.917079\n",
      "Epoch 22, CIFAR-10 Batch 5:  Validation Loss:     1.0215 Validation Accuracy: 0.677200\n",
      "Train Loss:     0.3415 Train Accuracy: 0.878713\n",
      "Epoch 23, CIFAR-10 Batch 1:  Validation Loss:     0.8781 Validation Accuracy: 0.720200\n",
      "Train Loss:     0.2968 Train Accuracy: 0.907178\n",
      "Epoch 23, CIFAR-10 Batch 2:  Validation Loss:     0.9435 Validation Accuracy: 0.702400\n",
      "Train Loss:     0.3108 Train Accuracy: 0.899752\n",
      "Epoch 23, CIFAR-10 Batch 3:  Validation Loss:     0.9332 Validation Accuracy: 0.705800\n",
      "Train Loss:     0.2952 Train Accuracy: 0.907178\n",
      "Epoch 23, CIFAR-10 Batch 4:  Validation Loss:     1.0684 Validation Accuracy: 0.677000\n",
      "Train Loss:     0.3626 Train Accuracy: 0.873762\n",
      "Epoch 23, CIFAR-10 Batch 5:  Validation Loss:     1.0073 Validation Accuracy: 0.682000\n",
      "Train Loss:     0.3019 Train Accuracy: 0.900990\n",
      "Epoch 24, CIFAR-10 Batch 1:  Validation Loss:     0.9087 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.2542 Train Accuracy: 0.923267\n",
      "Epoch 24, CIFAR-10 Batch 2:  Validation Loss:     1.0482 Validation Accuracy: 0.673000\n",
      "Train Loss:     0.3897 Train Accuracy: 0.865099\n",
      "Epoch 24, CIFAR-10 Batch 3:  Validation Loss:     0.9615 Validation Accuracy: 0.700200\n",
      "Train Loss:     0.2927 Train Accuracy: 0.898515\n",
      "Epoch 24, CIFAR-10 Batch 4:  Validation Loss:     0.9798 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.2597 Train Accuracy: 0.912129\n",
      "Epoch 24, CIFAR-10 Batch 5:  Validation Loss:     1.0492 Validation Accuracy: 0.684000\n",
      "Train Loss:     0.2667 Train Accuracy: 0.909653\n",
      "Epoch 25, CIFAR-10 Batch 1:  Validation Loss:     0.9264 Validation Accuracy: 0.709600\n",
      "Train Loss:     0.2396 Train Accuracy: 0.936881\n",
      "Epoch 25, CIFAR-10 Batch 2:  Validation Loss:     1.0624 Validation Accuracy: 0.673800\n",
      "Train Loss:     0.3552 Train Accuracy: 0.867574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, CIFAR-10 Batch 3:  Validation Loss:     0.9716 Validation Accuracy: 0.695600\n",
      "Train Loss:     0.2763 Train Accuracy: 0.904703\n",
      "Epoch 25, CIFAR-10 Batch 4:  Validation Loss:     0.9795 Validation Accuracy: 0.705400\n",
      "Train Loss:     0.2416 Train Accuracy: 0.917079\n",
      "Epoch 25, CIFAR-10 Batch 5:  Validation Loss:     1.0453 Validation Accuracy: 0.688600\n",
      "Train Loss:     0.2256 Train Accuracy: 0.926980\n",
      "Epoch 26, CIFAR-10 Batch 1:  Validation Loss:     0.9737 Validation Accuracy: 0.702800\n",
      "Train Loss:     0.2293 Train Accuracy: 0.934406\n",
      "Epoch 26, CIFAR-10 Batch 2:  Validation Loss:     1.0878 Validation Accuracy: 0.682600\n",
      "Train Loss:     0.3423 Train Accuracy: 0.877475\n",
      "Epoch 26, CIFAR-10 Batch 3:  Validation Loss:     0.9711 Validation Accuracy: 0.706000\n",
      "Train Loss:     0.2179 Train Accuracy: 0.933168\n",
      "Epoch 26, CIFAR-10 Batch 4:  Validation Loss:     0.9348 Validation Accuracy: 0.709000\n",
      "Train Loss:     0.1926 Train Accuracy: 0.946782\n",
      "Epoch 26, CIFAR-10 Batch 5:  Validation Loss:     1.0212 Validation Accuracy: 0.694200\n",
      "Train Loss:     0.1854 Train Accuracy: 0.946782\n",
      "Epoch 27, CIFAR-10 Batch 1:  Validation Loss:     0.9410 Validation Accuracy: 0.713600\n",
      "Train Loss:     0.1962 Train Accuracy: 0.946782\n",
      "Epoch 27, CIFAR-10 Batch 2:  Validation Loss:     1.1278 Validation Accuracy: 0.680600\n",
      "Train Loss:     0.2975 Train Accuracy: 0.909653\n",
      "Epoch 27, CIFAR-10 Batch 3:  Validation Loss:     1.0672 Validation Accuracy: 0.684400\n",
      "Train Loss:     0.2244 Train Accuracy: 0.926980\n",
      "Epoch 27, CIFAR-10 Batch 4:  Validation Loss:     0.9711 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.1751 Train Accuracy: 0.956683\n",
      "Epoch 27, CIFAR-10 Batch 5:  Validation Loss:     1.1110 Validation Accuracy: 0.685800\n",
      "Train Loss:     0.2060 Train Accuracy: 0.929455\n",
      "Epoch 28, CIFAR-10 Batch 1:  Validation Loss:     0.9658 Validation Accuracy: 0.708000\n",
      "Train Loss:     0.1901 Train Accuracy: 0.952970\n",
      "Epoch 28, CIFAR-10 Batch 2:  Validation Loss:     1.0049 Validation Accuracy: 0.702800\n",
      "Train Loss:     0.1986 Train Accuracy: 0.941832\n",
      "Epoch 28, CIFAR-10 Batch 3:  Validation Loss:     1.2043 Validation Accuracy: 0.659000\n",
      "Train Loss:     0.2555 Train Accuracy: 0.918317\n",
      "Epoch 28, CIFAR-10 Batch 4:  Validation Loss:     0.9684 Validation Accuracy: 0.695200\n",
      "Train Loss:     0.1830 Train Accuracy: 0.951733\n",
      "Epoch 28, CIFAR-10 Batch 5:  Validation Loss:     1.1280 Validation Accuracy: 0.677400\n",
      "Train Loss:     0.2187 Train Accuracy: 0.926980\n",
      "Epoch 29, CIFAR-10 Batch 1:  Validation Loss:     0.9919 Validation Accuracy: 0.707400\n",
      "Train Loss:     0.2223 Train Accuracy: 0.940594\n",
      "Epoch 29, CIFAR-10 Batch 2:  Validation Loss:     0.9951 Validation Accuracy: 0.702800\n",
      "Train Loss:     0.1812 Train Accuracy: 0.960396\n",
      "Epoch 29, CIFAR-10 Batch 3:  Validation Loss:     1.2311 Validation Accuracy: 0.661800\n",
      "Train Loss:     0.2447 Train Accuracy: 0.912129\n",
      "Epoch 29, CIFAR-10 Batch 4:  Validation Loss:     1.0024 Validation Accuracy: 0.696600\n",
      "Train Loss:     0.1906 Train Accuracy: 0.929455\n",
      "Epoch 29, CIFAR-10 Batch 5:  Validation Loss:     1.0185 Validation Accuracy: 0.704400\n",
      "Train Loss:     0.1389 Train Accuracy: 0.955445\n",
      "Epoch 30, CIFAR-10 Batch 1:  Validation Loss:     1.0395 Validation Accuracy: 0.700800\n",
      "Train Loss:     0.1835 Train Accuracy: 0.945544\n",
      "Epoch 30, CIFAR-10 Batch 2:  Validation Loss:     1.0096 Validation Accuracy: 0.713200\n",
      "Train Loss:     0.1397 Train Accuracy: 0.969059\n",
      "Epoch 30, CIFAR-10 Batch 3:  Validation Loss:     1.1104 Validation Accuracy: 0.689000\n",
      "Train Loss:     0.1599 Train Accuracy: 0.951733\n",
      "Epoch 30, CIFAR-10 Batch 4:  Validation Loss:     1.1137 Validation Accuracy: 0.684400\n",
      "Train Loss:     0.2137 Train Accuracy: 0.929455\n",
      "Epoch 30, CIFAR-10 Batch 5:  Validation Loss:     1.1161 Validation Accuracy: 0.690000\n",
      "Train Loss:     0.1595 Train Accuracy: 0.955446\n",
      "Epoch 31, CIFAR-10 Batch 1:  Validation Loss:     1.0166 Validation Accuracy: 0.704200\n",
      "Train Loss:     0.1571 Train Accuracy: 0.955446\n",
      "Epoch 31, CIFAR-10 Batch 2:  Validation Loss:     1.0204 Validation Accuracy: 0.717200\n",
      "Train Loss:     0.1210 Train Accuracy: 0.972772\n",
      "Epoch 31, CIFAR-10 Batch 3:  Validation Loss:     1.0819 Validation Accuracy: 0.698200\n",
      "Train Loss:     0.1359 Train Accuracy: 0.967822\n",
      "Epoch 31, CIFAR-10 Batch 4:  Validation Loss:     1.1168 Validation Accuracy: 0.688200\n",
      "Train Loss:     0.1608 Train Accuracy: 0.948020\n",
      "Epoch 31, CIFAR-10 Batch 5:  Validation Loss:     1.1377 Validation Accuracy: 0.699400\n",
      "Train Loss:     0.1289 Train Accuracy: 0.957921\n",
      "Epoch 32, CIFAR-10 Batch 1:  Validation Loss:     1.0504 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.1284 Train Accuracy: 0.969059\n",
      "Epoch 32, CIFAR-10 Batch 2:  Validation Loss:     1.0077 Validation Accuracy: 0.722800\n",
      "Train Loss:     0.1111 Train Accuracy: 0.975248\n",
      "Epoch 32, CIFAR-10 Batch 3:  Validation Loss:     1.1683 Validation Accuracy: 0.682800\n",
      "Train Loss:     0.1581 Train Accuracy: 0.954208\n",
      "Epoch 32, CIFAR-10 Batch 4:  Validation Loss:     1.0717 Validation Accuracy: 0.704600\n",
      "Train Loss:     0.1149 Train Accuracy: 0.970297\n",
      "Epoch 32, CIFAR-10 Batch 5:  Validation Loss:     1.1306 Validation Accuracy: 0.700200\n",
      "Train Loss:     0.1122 Train Accuracy: 0.970297\n",
      "Epoch 33, CIFAR-10 Batch 1:  Validation Loss:     1.0833 Validation Accuracy: 0.707200\n",
      "Train Loss:     0.1003 Train Accuracy: 0.982673\n",
      "Epoch 33, CIFAR-10 Batch 2:  Validation Loss:     1.0262 Validation Accuracy: 0.726400\n",
      "Train Loss:     0.0971 Train Accuracy: 0.975248\n",
      "Epoch 33, CIFAR-10 Batch 3:  Validation Loss:     1.1147 Validation Accuracy: 0.698400\n",
      "Train Loss:     0.1173 Train Accuracy: 0.960396\n",
      "Epoch 33, CIFAR-10 Batch 4:  Validation Loss:     1.1202 Validation Accuracy: 0.697200\n",
      "Train Loss:     0.1196 Train Accuracy: 0.965346\n",
      "Epoch 33, CIFAR-10 Batch 5:  Validation Loss:     1.1409 Validation Accuracy: 0.701200\n",
      "Train Loss:     0.0835 Train Accuracy: 0.983911\n",
      "Epoch 34, CIFAR-10 Batch 1:  Validation Loss:     1.1908 Validation Accuracy: 0.696200\n",
      "Train Loss:     0.1157 Train Accuracy: 0.965347\n",
      "Epoch 34, CIFAR-10 Batch 2:  Validation Loss:     1.0144 Validation Accuracy: 0.719000\n",
      "Train Loss:     0.0840 Train Accuracy: 0.985149\n",
      "Epoch 34, CIFAR-10 Batch 3:  Validation Loss:     1.0727 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.0867 Train Accuracy: 0.977723\n",
      "Epoch 34, CIFAR-10 Batch 4:  Validation Loss:     1.1266 Validation Accuracy: 0.695200\n",
      "Train Loss:     0.1160 Train Accuracy: 0.966584\n",
      "Epoch 34, CIFAR-10 Batch 5:  Validation Loss:     1.1521 Validation Accuracy: 0.704000\n",
      "Train Loss:     0.1186 Train Accuracy: 0.960396\n",
      "Epoch 35, CIFAR-10 Batch 1:  Validation Loss:     1.2105 Validation Accuracy: 0.697600\n",
      "Train Loss:     0.1213 Train Accuracy: 0.964109\n",
      "Epoch 35, CIFAR-10 Batch 2:  Validation Loss:     1.0618 Validation Accuracy: 0.711000\n",
      "Train Loss:     0.0960 Train Accuracy: 0.972772\n",
      "Epoch 35, CIFAR-10 Batch 3:  Validation Loss:     1.1150 Validation Accuracy: 0.701800\n",
      "Train Loss:     0.0742 Train Accuracy: 0.981436\n",
      "Epoch 35, CIFAR-10 Batch 4:  Validation Loss:     1.0584 Validation Accuracy: 0.706600\n",
      "Train Loss:     0.0744 Train Accuracy: 0.980198\n",
      "Epoch 35, CIFAR-10 Batch 5:  Validation Loss:     1.1898 Validation Accuracy: 0.700600\n",
      "Train Loss:     0.1042 Train Accuracy: 0.965347\n",
      "Epoch 36, CIFAR-10 Batch 1:  Validation Loss:     1.2858 Validation Accuracy: 0.687400\n",
      "Train Loss:     0.1521 Train Accuracy: 0.941832\n",
      "Epoch 36, CIFAR-10 Batch 2:  Validation Loss:     1.1144 Validation Accuracy: 0.711800\n",
      "Train Loss:     0.0998 Train Accuracy: 0.977723\n",
      "Epoch 36, CIFAR-10 Batch 3:  Validation Loss:     1.1342 Validation Accuracy: 0.699000\n",
      "Train Loss:     0.0713 Train Accuracy: 0.987624\n",
      "Epoch 36, CIFAR-10 Batch 4:  Validation Loss:     1.0649 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.0635 Train Accuracy: 0.987624\n",
      "Epoch 36, CIFAR-10 Batch 5:  Validation Loss:     1.1646 Validation Accuracy: 0.708400\n",
      "Train Loss:     0.0480 Train Accuracy: 0.990099\n",
      "Epoch 37, CIFAR-10 Batch 1:  Validation Loss:     1.2264 Validation Accuracy: 0.705000\n",
      "Train Loss:     0.0954 Train Accuracy: 0.977723\n",
      "Epoch 37, CIFAR-10 Batch 2:  Validation Loss:     1.1324 Validation Accuracy: 0.711800\n",
      "Train Loss:     0.0920 Train Accuracy: 0.978960\n",
      "Epoch 37, CIFAR-10 Batch 3:  Validation Loss:     1.1232 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.0562 Train Accuracy: 0.987624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, CIFAR-10 Batch 4:  Validation Loss:     1.0743 Validation Accuracy: 0.720800\n",
      "Train Loss:     0.0494 Train Accuracy: 0.988861\n",
      "Epoch 37, CIFAR-10 Batch 5:  Validation Loss:     1.1705 Validation Accuracy: 0.712600\n",
      "Train Loss:     0.0577 Train Accuracy: 0.992574\n",
      "Epoch 38, CIFAR-10 Batch 1:  Validation Loss:     1.1945 Validation Accuracy: 0.714000\n",
      "Train Loss:     0.0698 Train Accuracy: 0.985148\n",
      "Epoch 38, CIFAR-10 Batch 2:  Validation Loss:     1.2178 Validation Accuracy: 0.709400\n",
      "Train Loss:     0.0833 Train Accuracy: 0.975248\n",
      "Epoch 38, CIFAR-10 Batch 3:  Validation Loss:     1.2053 Validation Accuracy: 0.703200\n",
      "Train Loss:     0.0522 Train Accuracy: 0.987624\n",
      "Epoch 38, CIFAR-10 Batch 4:  Validation Loss:     1.1366 Validation Accuracy: 0.719400\n",
      "Train Loss:     0.0419 Train Accuracy: 0.991337\n",
      "Epoch 38, CIFAR-10 Batch 5:  Validation Loss:     1.1689 Validation Accuracy: 0.710200\n",
      "Train Loss:     0.0429 Train Accuracy: 0.995049\n",
      "Epoch 39, CIFAR-10 Batch 1:  Validation Loss:     1.2067 Validation Accuracy: 0.706400\n",
      "Train Loss:     0.0510 Train Accuracy: 0.988861\n",
      "Epoch 39, CIFAR-10 Batch 2:  Validation Loss:     1.1552 Validation Accuracy: 0.712800\n",
      "Train Loss:     0.0649 Train Accuracy: 0.982673\n",
      "Epoch 39, CIFAR-10 Batch 3:  Validation Loss:     1.1900 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.0406 Train Accuracy: 0.992574\n",
      "Epoch 39, CIFAR-10 Batch 4:  Validation Loss:     1.1477 Validation Accuracy: 0.717800\n",
      "Train Loss:     0.0427 Train Accuracy: 0.992574\n",
      "Epoch 39, CIFAR-10 Batch 5:  Validation Loss:     1.2076 Validation Accuracy: 0.709800\n",
      "Train Loss:     0.0436 Train Accuracy: 0.991337\n",
      "Epoch 40, CIFAR-10 Batch 1:  Validation Loss:     1.2257 Validation Accuracy: 0.720600\n",
      "Train Loss:     0.0432 Train Accuracy: 0.991337\n",
      "Epoch 40, CIFAR-10 Batch 2:  Validation Loss:     1.2302 Validation Accuracy: 0.708000\n",
      "Train Loss:     0.0776 Train Accuracy: 0.978960\n",
      "Epoch 40, CIFAR-10 Batch 3:  Validation Loss:     1.2244 Validation Accuracy: 0.705000\n",
      "Train Loss:     0.0501 Train Accuracy: 0.981436\n",
      "Epoch 40, CIFAR-10 Batch 4:  Validation Loss:     1.1003 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0264 Train Accuracy: 0.997525\n",
      "Epoch 40, CIFAR-10 Batch 5:  Validation Loss:     1.1516 Validation Accuracy: 0.716800\n",
      "Train Loss:     0.0274 Train Accuracy: 0.997525\n",
      "Epoch 41, CIFAR-10 Batch 1:  Validation Loss:     1.2280 Validation Accuracy: 0.716800\n",
      "Train Loss:     0.0518 Train Accuracy: 0.988861\n",
      "Epoch 41, CIFAR-10 Batch 2:  Validation Loss:     1.2404 Validation Accuracy: 0.698600\n",
      "Train Loss:     0.0728 Train Accuracy: 0.980198\n",
      "Epoch 41, CIFAR-10 Batch 3:  Validation Loss:     1.2768 Validation Accuracy: 0.691400\n",
      "Train Loss:     0.0520 Train Accuracy: 0.982673\n",
      "Epoch 41, CIFAR-10 Batch 4:  Validation Loss:     1.1341 Validation Accuracy: 0.719600\n",
      "Train Loss:     0.0395 Train Accuracy: 0.995049\n",
      "Epoch 41, CIFAR-10 Batch 5:  Validation Loss:     1.1548 Validation Accuracy: 0.714200\n",
      "Train Loss:     0.0329 Train Accuracy: 0.993812\n",
      "Epoch 42, CIFAR-10 Batch 1:  Validation Loss:     1.2414 Validation Accuracy: 0.718400\n",
      "Train Loss:     0.0552 Train Accuracy: 0.983911\n",
      "Epoch 42, CIFAR-10 Batch 2:  Validation Loss:     1.3631 Validation Accuracy: 0.680600\n",
      "Train Loss:     0.1131 Train Accuracy: 0.960396\n",
      "Epoch 42, CIFAR-10 Batch 3:  Validation Loss:     1.2177 Validation Accuracy: 0.705200\n",
      "Train Loss:     0.0357 Train Accuracy: 0.991337\n",
      "Epoch 42, CIFAR-10 Batch 4:  Validation Loss:     1.1961 Validation Accuracy: 0.718200\n",
      "Train Loss:     0.0437 Train Accuracy: 0.991337\n",
      "Epoch 42, CIFAR-10 Batch 5:  Validation Loss:     1.1766 Validation Accuracy: 0.710200\n",
      "Train Loss:     0.0529 Train Accuracy: 0.985149\n",
      "Epoch 43, CIFAR-10 Batch 1:  Validation Loss:     1.1826 Validation Accuracy: 0.722200\n",
      "Train Loss:     0.0505 Train Accuracy: 0.991337\n",
      "Epoch 43, CIFAR-10 Batch 2:  Validation Loss:     1.2211 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.0323 Train Accuracy: 0.993812\n",
      "Epoch 43, CIFAR-10 Batch 3:  Validation Loss:     1.1583 Validation Accuracy: 0.716400\n",
      "Train Loss:     0.0313 Train Accuracy: 0.993812\n",
      "Epoch 43, CIFAR-10 Batch 4:  Validation Loss:     1.2257 Validation Accuracy: 0.712600\n",
      "Train Loss:     0.0366 Train Accuracy: 0.993812\n",
      "Epoch 43, CIFAR-10 Batch 5:  Validation Loss:     1.1487 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0358 Train Accuracy: 0.992574\n",
      "Epoch 44, CIFAR-10 Batch 1:  Validation Loss:     1.1720 Validation Accuracy: 0.730600\n",
      "Train Loss:     0.0383 Train Accuracy: 0.993812\n",
      "Epoch 44, CIFAR-10 Batch 2:  Validation Loss:     1.1599 Validation Accuracy: 0.724600\n",
      "Train Loss:     0.0341 Train Accuracy: 0.993812\n",
      "Epoch 44, CIFAR-10 Batch 3:  Validation Loss:     1.1588 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.0334 Train Accuracy: 0.995049\n",
      "Epoch 44, CIFAR-10 Batch 4:  Validation Loss:     1.2767 Validation Accuracy: 0.708200\n",
      "Train Loss:     0.0316 Train Accuracy: 0.995049\n",
      "Epoch 44, CIFAR-10 Batch 5:  Validation Loss:     1.1462 Validation Accuracy: 0.713000\n",
      "Train Loss:     0.0393 Train Accuracy: 0.995049\n",
      "Epoch 45, CIFAR-10 Batch 1:  Validation Loss:     1.1682 Validation Accuracy: 0.722600\n",
      "Train Loss:     0.0339 Train Accuracy: 0.991337\n",
      "Epoch 45, CIFAR-10 Batch 2:  Validation Loss:     1.1901 Validation Accuracy: 0.719400\n",
      "Train Loss:     0.0305 Train Accuracy: 0.993812\n",
      "Epoch 45, CIFAR-10 Batch 3:  Validation Loss:     1.2515 Validation Accuracy: 0.703400\n",
      "Train Loss:     0.0278 Train Accuracy: 0.996287\n",
      "Epoch 45, CIFAR-10 Batch 4:  Validation Loss:     1.2161 Validation Accuracy: 0.718000\n",
      "Train Loss:     0.0212 Train Accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 5:  Validation Loss:     1.1825 Validation Accuracy: 0.723000\n",
      "Train Loss:     0.0300 Train Accuracy: 0.996287\n",
      "Epoch 46, CIFAR-10 Batch 1:  Validation Loss:     1.2059 Validation Accuracy: 0.722800\n",
      "Train Loss:     0.0364 Train Accuracy: 0.993812\n",
      "Epoch 46, CIFAR-10 Batch 2:  Validation Loss:     1.1819 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0232 Train Accuracy: 0.997525\n",
      "Epoch 46, CIFAR-10 Batch 3:  Validation Loss:     1.2474 Validation Accuracy: 0.704800\n",
      "Train Loss:     0.0311 Train Accuracy: 0.995049\n",
      "Epoch 46, CIFAR-10 Batch 4:  Validation Loss:     1.2566 Validation Accuracy: 0.714000\n",
      "Train Loss:     0.0328 Train Accuracy: 0.991337\n",
      "Epoch 46, CIFAR-10 Batch 5:  Validation Loss:     1.2458 Validation Accuracy: 0.710800\n",
      "Train Loss:     0.0442 Train Accuracy: 0.991337\n",
      "Epoch 47, CIFAR-10 Batch 1:  Validation Loss:     1.2186 Validation Accuracy: 0.712200\n",
      "Train Loss:     0.0386 Train Accuracy: 0.995049\n",
      "Epoch 47, CIFAR-10 Batch 2:  Validation Loss:     1.1783 Validation Accuracy: 0.719000\n",
      "Train Loss:     0.0381 Train Accuracy: 0.987624\n",
      "Epoch 47, CIFAR-10 Batch 3:  Validation Loss:     1.2470 Validation Accuracy: 0.703200\n",
      "Train Loss:     0.0323 Train Accuracy: 0.995049\n",
      "Epoch 47, CIFAR-10 Batch 4:  Validation Loss:     1.1574 Validation Accuracy: 0.721800\n",
      "Train Loss:     0.0218 Train Accuracy: 0.998762\n",
      "Epoch 47, CIFAR-10 Batch 5:  Validation Loss:     1.2629 Validation Accuracy: 0.716600\n",
      "Train Loss:     0.0277 Train Accuracy: 0.996287\n",
      "Epoch 48, CIFAR-10 Batch 1:  Validation Loss:     1.2155 Validation Accuracy: 0.711600\n",
      "Train Loss:     0.0341 Train Accuracy: 0.996287\n",
      "Epoch 48, CIFAR-10 Batch 2:  Validation Loss:     1.1459 Validation Accuracy: 0.716600\n",
      "Train Loss:     0.0346 Train Accuracy: 0.995049\n",
      "Epoch 48, CIFAR-10 Batch 3:  Validation Loss:     1.2416 Validation Accuracy: 0.713400\n",
      "Train Loss:     0.0282 Train Accuracy: 0.996287\n",
      "Epoch 48, CIFAR-10 Batch 4:  Validation Loss:     1.1741 Validation Accuracy: 0.721800\n",
      "Train Loss:     0.0240 Train Accuracy: 0.997525\n",
      "Epoch 48, CIFAR-10 Batch 5:  Validation Loss:     1.1680 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0158 Train Accuracy: 0.998762\n",
      "Epoch 49, CIFAR-10 Batch 1:  Validation Loss:     1.3173 Validation Accuracy: 0.702000\n",
      "Train Loss:     0.0360 Train Accuracy: 0.990099\n",
      "Epoch 49, CIFAR-10 Batch 2:  Validation Loss:     1.2870 Validation Accuracy: 0.704400\n",
      "Train Loss:     0.0482 Train Accuracy: 0.985149\n",
      "Epoch 49, CIFAR-10 Batch 3:  Validation Loss:     1.2854 Validation Accuracy: 0.705400\n",
      "Train Loss:     0.0487 Train Accuracy: 0.992574\n",
      "Epoch 49, CIFAR-10 Batch 4:  Validation Loss:     1.2495 Validation Accuracy: 0.705200\n",
      "Train Loss:     0.0363 Train Accuracy: 0.991337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, CIFAR-10 Batch 5:  Validation Loss:     1.2723 Validation Accuracy: 0.710800\n",
      "Train Loss:     0.0381 Train Accuracy: 0.991337\n",
      "Epoch 50, CIFAR-10 Batch 1:  Validation Loss:     1.2746 Validation Accuracy: 0.715200\n",
      "Train Loss:     0.0347 Train Accuracy: 0.992574\n",
      "Epoch 50, CIFAR-10 Batch 2:  Validation Loss:     1.3732 Validation Accuracy: 0.700800\n",
      "Train Loss:     0.0471 Train Accuracy: 0.988861\n",
      "Epoch 50, CIFAR-10 Batch 3:  Validation Loss:     1.2857 Validation Accuracy: 0.696000\n",
      "Train Loss:     0.0363 Train Accuracy: 0.992574\n",
      "Epoch 50, CIFAR-10 Batch 4:  Validation Loss:     1.2366 Validation Accuracy: 0.712800\n",
      "Train Loss:     0.0278 Train Accuracy: 0.996287\n",
      "Epoch 50, CIFAR-10 Batch 5:  Validation Loss:     1.2457 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.0159 Train Accuracy: 0.998762\n",
      "Epoch 51, CIFAR-10 Batch 1:  Validation Loss:     1.2714 Validation Accuracy: 0.717000\n",
      "Train Loss:     0.0196 Train Accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Validation Loss:     1.3626 Validation Accuracy: 0.718000\n",
      "Train Loss:     0.0235 Train Accuracy: 0.995049\n",
      "Epoch 51, CIFAR-10 Batch 3:  Validation Loss:     1.3410 Validation Accuracy: 0.700000\n",
      "Train Loss:     0.0278 Train Accuracy: 0.995049\n",
      "Epoch 51, CIFAR-10 Batch 4:  Validation Loss:     1.2379 Validation Accuracy: 0.716400\n",
      "Train Loss:     0.0310 Train Accuracy: 0.993812\n",
      "Epoch 51, CIFAR-10 Batch 5:  Validation Loss:     1.2308 Validation Accuracy: 0.726800\n",
      "Train Loss:     0.0162 Train Accuracy: 0.997525\n",
      "Epoch 52, CIFAR-10 Batch 1:  Validation Loss:     1.2683 Validation Accuracy: 0.719600\n",
      "Train Loss:     0.0229 Train Accuracy: 0.998762\n",
      "Epoch 52, CIFAR-10 Batch 2:  Validation Loss:     1.2788 Validation Accuracy: 0.718000\n",
      "Train Loss:     0.0188 Train Accuracy: 0.998762\n",
      "Epoch 52, CIFAR-10 Batch 3:  Validation Loss:     1.2993 Validation Accuracy: 0.709200\n",
      "Train Loss:     0.0172 Train Accuracy: 0.998762\n",
      "Epoch 52, CIFAR-10 Batch 4:  Validation Loss:     1.2323 Validation Accuracy: 0.721800\n",
      "Train Loss:     0.0209 Train Accuracy: 0.996287\n",
      "Epoch 52, CIFAR-10 Batch 5:  Validation Loss:     1.2486 Validation Accuracy: 0.721400\n",
      "Train Loss:     0.0147 Train Accuracy: 0.997525\n",
      "Epoch 53, CIFAR-10 Batch 1:  Validation Loss:     1.3439 Validation Accuracy: 0.715600\n",
      "Train Loss:     0.0235 Train Accuracy: 0.995049\n",
      "Epoch 53, CIFAR-10 Batch 2:  Validation Loss:     1.3912 Validation Accuracy: 0.719200\n",
      "Train Loss:     0.0195 Train Accuracy: 0.998762\n",
      "Epoch 53, CIFAR-10 Batch 3:  Validation Loss:     1.3199 Validation Accuracy: 0.716600\n",
      "Train Loss:     0.0189 Train Accuracy: 0.996287\n",
      "Epoch 53, CIFAR-10 Batch 4:  Validation Loss:     1.2777 Validation Accuracy: 0.724800\n",
      "Train Loss:     0.0204 Train Accuracy: 0.996287\n",
      "Epoch 53, CIFAR-10 Batch 5:  Validation Loss:     1.3701 Validation Accuracy: 0.714000\n",
      "Train Loss:     0.0158 Train Accuracy: 0.997525\n",
      "Epoch 54, CIFAR-10 Batch 1:  Validation Loss:     1.4209 Validation Accuracy: 0.710800\n",
      "Train Loss:     0.0235 Train Accuracy: 0.996287\n",
      "Epoch 54, CIFAR-10 Batch 2:  Validation Loss:     1.3770 Validation Accuracy: 0.717400\n",
      "Train Loss:     0.0249 Train Accuracy: 0.993812\n",
      "Epoch 54, CIFAR-10 Batch 3:  Validation Loss:     1.3990 Validation Accuracy: 0.713200\n",
      "Train Loss:     0.0229 Train Accuracy: 0.995049\n",
      "Epoch 54, CIFAR-10 Batch 4:  Validation Loss:     1.2934 Validation Accuracy: 0.722400\n",
      "Train Loss:     0.0175 Train Accuracy: 0.995049\n",
      "Epoch 54, CIFAR-10 Batch 5:  Validation Loss:     1.3932 Validation Accuracy: 0.704200\n",
      "Train Loss:     0.0174 Train Accuracy: 0.998762\n",
      "Epoch 55, CIFAR-10 Batch 1:  Validation Loss:     1.4269 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.0237 Train Accuracy: 0.995049\n",
      "Epoch 55, CIFAR-10 Batch 2:  Validation Loss:     1.4000 Validation Accuracy: 0.718200\n",
      "Train Loss:     0.0282 Train Accuracy: 0.993812\n",
      "Epoch 55, CIFAR-10 Batch 3:  Validation Loss:     1.3967 Validation Accuracy: 0.715600\n",
      "Train Loss:     0.0211 Train Accuracy: 0.995049\n",
      "Epoch 55, CIFAR-10 Batch 4:  Validation Loss:     1.3157 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.0197 Train Accuracy: 0.995049\n",
      "Epoch 55, CIFAR-10 Batch 5:  Validation Loss:     1.4324 Validation Accuracy: 0.702200\n",
      "Train Loss:     0.0221 Train Accuracy: 0.995049\n",
      "Epoch 56, CIFAR-10 Batch 1:  Validation Loss:     1.4083 Validation Accuracy: 0.714400\n",
      "Train Loss:     0.0153 Train Accuracy: 0.997525\n",
      "Epoch 56, CIFAR-10 Batch 2:  Validation Loss:     1.3284 Validation Accuracy: 0.729600\n",
      "Train Loss:     0.0135 Train Accuracy: 0.997525\n",
      "Epoch 56, CIFAR-10 Batch 3:  Validation Loss:     1.3867 Validation Accuracy: 0.716800\n",
      "Train Loss:     0.0131 Train Accuracy: 0.997525\n",
      "Epoch 56, CIFAR-10 Batch 4:  Validation Loss:     1.3268 Validation Accuracy: 0.723200\n",
      "Train Loss:     0.0160 Train Accuracy: 0.997525\n",
      "Epoch 56, CIFAR-10 Batch 5:  Validation Loss:     1.3665 Validation Accuracy: 0.718200\n",
      "Train Loss:     0.0107 Train Accuracy: 0.998762\n",
      "Epoch 57, CIFAR-10 Batch 1:  Validation Loss:     1.3577 Validation Accuracy: 0.728400\n",
      "Train Loss:     0.0111 Train Accuracy: 0.998762\n",
      "Epoch 57, CIFAR-10 Batch 2:  Validation Loss:     1.3525 Validation Accuracy: 0.727200\n",
      "Train Loss:     0.0108 Train Accuracy: 0.998762\n",
      "Epoch 57, CIFAR-10 Batch 3:  Validation Loss:     1.4307 Validation Accuracy: 0.719600\n",
      "Train Loss:     0.0112 Train Accuracy: 0.998762\n",
      "Epoch 57, CIFAR-10 Batch 4:  Validation Loss:     1.3088 Validation Accuracy: 0.731000\n",
      "Train Loss:     0.0096 Train Accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 5:  Validation Loss:     1.3393 Validation Accuracy: 0.727200\n",
      "Train Loss:     0.0078 Train Accuracy: 0.997525\n",
      "Epoch 58, CIFAR-10 Batch 1:  Validation Loss:     1.3670 Validation Accuracy: 0.723000\n",
      "Train Loss:     0.0087 Train Accuracy: 0.998762\n",
      "Epoch 58, CIFAR-10 Batch 2:  Validation Loss:     1.3703 Validation Accuracy: 0.729200\n",
      "Train Loss:     0.0100 Train Accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Validation Loss:     1.3999 Validation Accuracy: 0.724200\n",
      "Train Loss:     0.0067 Train Accuracy: 0.998762\n",
      "Epoch 58, CIFAR-10 Batch 4:  Validation Loss:     1.3651 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0086 Train Accuracy: 0.998762\n",
      "Epoch 58, CIFAR-10 Batch 5:  Validation Loss:     1.4462 Validation Accuracy: 0.724000\n",
      "Train Loss:     0.0119 Train Accuracy: 0.998762\n",
      "Epoch 59, CIFAR-10 Batch 1:  Validation Loss:     1.5037 Validation Accuracy: 0.713600\n",
      "Train Loss:     0.0169 Train Accuracy: 0.995049\n",
      "Epoch 59, CIFAR-10 Batch 2:  Validation Loss:     1.4405 Validation Accuracy: 0.717800\n",
      "Train Loss:     0.0101 Train Accuracy: 0.998762\n",
      "Epoch 59, CIFAR-10 Batch 3:  Validation Loss:     1.4521 Validation Accuracy: 0.719000\n",
      "Train Loss:     0.0120 Train Accuracy: 0.996287\n",
      "Epoch 59, CIFAR-10 Batch 4:  Validation Loss:     1.3884 Validation Accuracy: 0.721200\n",
      "Train Loss:     0.0087 Train Accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 5:  Validation Loss:     1.4024 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0076 Train Accuracy: 0.998762\n",
      "Epoch 60, CIFAR-10 Batch 1:  Validation Loss:     1.4680 Validation Accuracy: 0.722000\n",
      "Train Loss:     0.0166 Train Accuracy: 0.997525\n",
      "Epoch 60, CIFAR-10 Batch 2:  Validation Loss:     1.4022 Validation Accuracy: 0.726800\n",
      "Train Loss:     0.0087 Train Accuracy: 0.998762\n",
      "Epoch 60, CIFAR-10 Batch 3:  Validation Loss:     1.4188 Validation Accuracy: 0.722200\n",
      "Train Loss:     0.0064 Train Accuracy: 0.998762\n",
      "Epoch 60, CIFAR-10 Batch 4:  Validation Loss:     1.3886 Validation Accuracy: 0.725000\n",
      "Train Loss:     0.0088 Train Accuracy: 0.998762\n",
      "Epoch 60, CIFAR-10 Batch 5:  Validation Loss:     1.4352 Validation Accuracy: 0.715600\n",
      "Train Loss:     0.0062 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 1:  Validation Loss:     1.4339 Validation Accuracy: 0.722800\n",
      "Train Loss:     0.0080 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 2:  Validation Loss:     1.3702 Validation Accuracy: 0.727800\n",
      "Train Loss:     0.0079 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 3:  Validation Loss:     1.4629 Validation Accuracy: 0.721000\n",
      "Train Loss:     0.0063 Train Accuracy: 0.998762\n",
      "Epoch 61, CIFAR-10 Batch 4:  Validation Loss:     1.3456 Validation Accuracy: 0.727600\n",
      "Train Loss:     0.0073 Train Accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 5:  Validation Loss:     1.4279 Validation Accuracy: 0.724800\n",
      "Train Loss:     0.0119 Train Accuracy: 0.996287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, CIFAR-10 Batch 1:  Validation Loss:     1.4897 Validation Accuracy: 0.723400\n",
      "Train Loss:     0.0086 Train Accuracy: 0.998762\n",
      "Epoch 62, CIFAR-10 Batch 2:  Validation Loss:     1.4049 Validation Accuracy: 0.733600\n",
      "Train Loss:     0.0062 Train Accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 3:  Validation Loss:     1.4868 Validation Accuracy: 0.717800\n",
      "Train Loss:     0.0100 Train Accuracy: 0.998762\n",
      "Epoch 62, CIFAR-10 Batch 4:  Validation Loss:     1.4248 Validation Accuracy: 0.718200\n",
      "Train Loss:     0.0083 Train Accuracy: 0.998762\n",
      "Epoch 62, CIFAR-10 Batch 5:  Validation Loss:     1.4915 Validation Accuracy: 0.719600\n",
      "Train Loss:     0.0079 Train Accuracy: 0.998762\n",
      "Epoch 63, CIFAR-10 Batch 1:  Validation Loss:     1.4929 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0070 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 2:  Validation Loss:     1.3986 Validation Accuracy: 0.730000\n",
      "Train Loss:     0.0077 Train Accuracy: 0.998762\n",
      "Epoch 63, CIFAR-10 Batch 3:  Validation Loss:     1.4388 Validation Accuracy: 0.724800\n",
      "Train Loss:     0.0045 Train Accuracy: 0.998762\n",
      "Epoch 63, CIFAR-10 Batch 4:  Validation Loss:     1.4171 Validation Accuracy: 0.719200\n",
      "Train Loss:     0.0063 Train Accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Validation Loss:     1.4111 Validation Accuracy: 0.727400\n",
      "Train Loss:     0.0031 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Validation Loss:     1.5358 Validation Accuracy: 0.720800\n",
      "Train Loss:     0.0088 Train Accuracy: 0.998762\n",
      "Epoch 64, CIFAR-10 Batch 2:  Validation Loss:     1.4549 Validation Accuracy: 0.726800\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Validation Loss:     1.4820 Validation Accuracy: 0.720800\n",
      "Train Loss:     0.0080 Train Accuracy: 0.998762\n",
      "Epoch 64, CIFAR-10 Batch 4:  Validation Loss:     1.4231 Validation Accuracy: 0.727200\n",
      "Train Loss:     0.0042 Train Accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 5:  Validation Loss:     1.4818 Validation Accuracy: 0.708800\n",
      "Train Loss:     0.0059 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Validation Loss:     1.5145 Validation Accuracy: 0.718000\n",
      "Train Loss:     0.0105 Train Accuracy: 0.997525\n",
      "Epoch 65, CIFAR-10 Batch 2:  Validation Loss:     1.4133 Validation Accuracy: 0.730000\n",
      "Train Loss:     0.0057 Train Accuracy: 0.998762\n",
      "Epoch 65, CIFAR-10 Batch 3:  Validation Loss:     1.5035 Validation Accuracy: 0.713000\n",
      "Train Loss:     0.0056 Train Accuracy: 0.998762\n",
      "Epoch 65, CIFAR-10 Batch 4:  Validation Loss:     1.4511 Validation Accuracy: 0.718600\n",
      "Train Loss:     0.0068 Train Accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 5:  Validation Loss:     1.4529 Validation Accuracy: 0.715400\n",
      "Train Loss:     0.0061 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  Validation Loss:     1.5191 Validation Accuracy: 0.716600\n",
      "Train Loss:     0.0092 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Validation Loss:     1.4072 Validation Accuracy: 0.728600\n",
      "Train Loss:     0.0059 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 3:  Validation Loss:     1.5013 Validation Accuracy: 0.712600\n",
      "Train Loss:     0.0084 Train Accuracy: 0.997525\n",
      "Epoch 66, CIFAR-10 Batch 4:  Validation Loss:     1.3957 Validation Accuracy: 0.720600\n",
      "Train Loss:     0.0058 Train Accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 5:  Validation Loss:     1.5221 Validation Accuracy: 0.709000\n",
      "Train Loss:     0.0098 Train Accuracy: 0.998762\n",
      "Epoch 67, CIFAR-10 Batch 1:  Validation Loss:     1.5652 Validation Accuracy: 0.715200\n",
      "Train Loss:     0.0080 Train Accuracy: 0.998762\n",
      "Epoch 67, CIFAR-10 Batch 2:  Validation Loss:     1.3591 Validation Accuracy: 0.728600\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 3:  Validation Loss:     1.5212 Validation Accuracy: 0.709200\n",
      "Train Loss:     0.0062 Train Accuracy: 0.998762\n",
      "Epoch 67, CIFAR-10 Batch 4:  Validation Loss:     1.3978 Validation Accuracy: 0.726600\n",
      "Train Loss:     0.0082 Train Accuracy: 0.998762\n",
      "Epoch 67, CIFAR-10 Batch 5:  Validation Loss:     1.5134 Validation Accuracy: 0.705600\n",
      "Train Loss:     0.0107 Train Accuracy: 0.997525\n",
      "Epoch 68, CIFAR-10 Batch 1:  Validation Loss:     1.4699 Validation Accuracy: 0.726400\n",
      "Train Loss:     0.0092 Train Accuracy: 0.998762\n",
      "Epoch 68, CIFAR-10 Batch 2:  Validation Loss:     1.4141 Validation Accuracy: 0.725000\n",
      "Train Loss:     0.0055 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 3:  Validation Loss:     1.5201 Validation Accuracy: 0.708800\n",
      "Train Loss:     0.0062 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Validation Loss:     1.4727 Validation Accuracy: 0.721000\n",
      "Train Loss:     0.0060 Train Accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 5:  Validation Loss:     1.5954 Validation Accuracy: 0.704800\n",
      "Train Loss:     0.0207 Train Accuracy: 0.991337\n",
      "Epoch 69, CIFAR-10 Batch 1:  Validation Loss:     1.4003 Validation Accuracy: 0.730400\n",
      "Train Loss:     0.0027 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 2:  Validation Loss:     1.4416 Validation Accuracy: 0.720400\n",
      "Train Loss:     0.0051 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 3:  Validation Loss:     1.4452 Validation Accuracy: 0.726000\n",
      "Train Loss:     0.0052 Train Accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Validation Loss:     1.5415 Validation Accuracy: 0.710800\n",
      "Train Loss:     0.0193 Train Accuracy: 0.993812\n",
      "Epoch 69, CIFAR-10 Batch 5:  Validation Loss:     1.4892 Validation Accuracy: 0.703400\n",
      "Train Loss:     0.0143 Train Accuracy: 0.997525\n",
      "Epoch 70, CIFAR-10 Batch 1:  Validation Loss:     1.4051 Validation Accuracy: 0.719200\n",
      "Train Loss:     0.0043 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Validation Loss:     1.4215 Validation Accuracy: 0.714400\n",
      "Train Loss:     0.0068 Train Accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Validation Loss:     1.5457 Validation Accuracy: 0.706800\n",
      "Train Loss:     0.0102 Train Accuracy: 0.997525\n",
      "Epoch 70, CIFAR-10 Batch 4:  Validation Loss:     1.5921 Validation Accuracy: 0.703800\n",
      "Train Loss:     0.0202 Train Accuracy: 0.995049\n",
      "Epoch 70, CIFAR-10 Batch 5:  Validation Loss:     1.3800 Validation Accuracy: 0.725800\n",
      "Train Loss:     0.0070 Train Accuracy: 0.998762\n",
      "Epoch 71, CIFAR-10 Batch 1:  Validation Loss:     1.4256 Validation Accuracy: 0.718400\n",
      "Train Loss:     0.0073 Train Accuracy: 0.998762\n",
      "Epoch 71, CIFAR-10 Batch 2:  Validation Loss:     1.3738 Validation Accuracy: 0.722600\n",
      "Train Loss:     0.0095 Train Accuracy: 0.998762\n",
      "Epoch 71, CIFAR-10 Batch 3:  Validation Loss:     1.5670 Validation Accuracy: 0.712600\n",
      "Train Loss:     0.0114 Train Accuracy: 0.996287\n",
      "Epoch 71, CIFAR-10 Batch 4:  Validation Loss:     1.5015 Validation Accuracy: 0.714400\n",
      "Train Loss:     0.0095 Train Accuracy: 0.998762\n",
      "Epoch 71, CIFAR-10 Batch 5:  Validation Loss:     1.3254 Validation Accuracy: 0.733800\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Validation Loss:     1.4682 Validation Accuracy: 0.718400\n",
      "Train Loss:     0.0078 Train Accuracy: 0.998762\n",
      "Epoch 72, CIFAR-10 Batch 2:  Validation Loss:     1.3661 Validation Accuracy: 0.727000\n",
      "Train Loss:     0.0052 Train Accuracy: 0.998762\n",
      "Epoch 72, CIFAR-10 Batch 3:  Validation Loss:     1.6100 Validation Accuracy: 0.705800\n",
      "Train Loss:     0.0132 Train Accuracy: 0.993812\n",
      "Epoch 72, CIFAR-10 Batch 4:  Validation Loss:     1.4863 Validation Accuracy: 0.718600\n",
      "Train Loss:     0.0047 Train Accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 5:  Validation Loss:     1.3387 Validation Accuracy: 0.739600\n",
      "Train Loss:     0.0031 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 1:  Validation Loss:     1.4242 Validation Accuracy: 0.724800\n",
      "Train Loss:     0.0057 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 2:  Validation Loss:     1.3737 Validation Accuracy: 0.734000\n",
      "Train Loss:     0.0049 Train Accuracy: 0.998762\n",
      "Epoch 73, CIFAR-10 Batch 3:  Validation Loss:     1.5505 Validation Accuracy: 0.715400\n",
      "Train Loss:     0.0042 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Validation Loss:     1.4816 Validation Accuracy: 0.725600\n",
      "Train Loss:     0.0042 Train Accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Validation Loss:     1.3216 Validation Accuracy: 0.736600\n",
      "Train Loss:     0.0031 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Validation Loss:     1.4871 Validation Accuracy: 0.713400\n",
      "Train Loss:     0.0047 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, CIFAR-10 Batch 2:  Validation Loss:     1.4220 Validation Accuracy: 0.728200\n",
      "Train Loss:     0.0023 Train Accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 3:  Validation Loss:     1.6124 Validation Accuracy: 0.704400\n",
      "Train Loss:     0.0112 Train Accuracy: 0.995049\n",
      "Epoch 74, CIFAR-10 Batch 4:  Validation Loss:     1.5124 Validation Accuracy: 0.716000\n",
      "Train Loss:     0.0076 Train Accuracy: 0.998762\n",
      "Epoch 74, CIFAR-10 Batch 5:  Validation Loss:     1.3883 Validation Accuracy: 0.731800\n",
      "Train Loss:     0.0022 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Validation Loss:     1.5829 Validation Accuracy: 0.714000\n",
      "Train Loss:     0.0047 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 2:  Validation Loss:     1.5139 Validation Accuracy: 0.731800\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 3:  Validation Loss:     1.6248 Validation Accuracy: 0.713600\n",
      "Train Loss:     0.0064 Train Accuracy: 0.998762\n",
      "Epoch 75, CIFAR-10 Batch 4:  Validation Loss:     1.5360 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0037 Train Accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 5:  Validation Loss:     1.4252 Validation Accuracy: 0.733600\n",
      "Train Loss:     0.0039 Train Accuracy: 0.998762\n",
      "Epoch 76, CIFAR-10 Batch 1:  Validation Loss:     1.5933 Validation Accuracy: 0.710000\n",
      "Train Loss:     0.0062 Train Accuracy: 0.997525\n",
      "Epoch 76, CIFAR-10 Batch 2:  Validation Loss:     1.5720 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0052 Train Accuracy: 0.998762\n",
      "Epoch 76, CIFAR-10 Batch 3:  Validation Loss:     1.6430 Validation Accuracy: 0.706400\n",
      "Train Loss:     0.0029 Train Accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 4:  Validation Loss:     1.5684 Validation Accuracy: 0.717600\n",
      "Train Loss:     0.0056 Train Accuracy: 0.997525\n",
      "Epoch 76, CIFAR-10 Batch 5:  Validation Loss:     1.4584 Validation Accuracy: 0.728400\n",
      "Train Loss:     0.0027 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  Validation Loss:     1.6046 Validation Accuracy: 0.713600\n",
      "Train Loss:     0.0104 Train Accuracy: 0.997525\n",
      "Epoch 77, CIFAR-10 Batch 2:  Validation Loss:     1.4991 Validation Accuracy: 0.727000\n",
      "Train Loss:     0.0030 Train Accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Validation Loss:     1.5864 Validation Accuracy: 0.716200\n",
      "Train Loss:     0.0063 Train Accuracy: 0.998762\n",
      "Epoch 77, CIFAR-10 Batch 4:  Validation Loss:     1.5341 Validation Accuracy: 0.724000\n",
      "Train Loss:     0.0032 Train Accuracy: 0.998762\n",
      "Epoch 77, CIFAR-10 Batch 5:  Validation Loss:     1.4297 Validation Accuracy: 0.731400\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  Validation Loss:     1.6276 Validation Accuracy: 0.713200\n",
      "Train Loss:     0.0041 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Validation Loss:     1.4947 Validation Accuracy: 0.729000\n",
      "Train Loss:     0.0041 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 3:  Validation Loss:     1.6890 Validation Accuracy: 0.709200\n",
      "Train Loss:     0.0053 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 4:  Validation Loss:     1.5202 Validation Accuracy: 0.724000\n",
      "Train Loss:     0.0029 Train Accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 5:  Validation Loss:     1.4777 Validation Accuracy: 0.735600\n",
      "Train Loss:     0.0032 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  Validation Loss:     1.6414 Validation Accuracy: 0.713000\n",
      "Train Loss:     0.0056 Train Accuracy: 0.998762\n",
      "Epoch 79, CIFAR-10 Batch 2:  Validation Loss:     1.5017 Validation Accuracy: 0.733600\n",
      "Train Loss:     0.0039 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 3:  Validation Loss:     1.6722 Validation Accuracy: 0.711200\n",
      "Train Loss:     0.0054 Train Accuracy: 0.998762\n",
      "Epoch 79, CIFAR-10 Batch 4:  Validation Loss:     1.5302 Validation Accuracy: 0.726600\n",
      "Train Loss:     0.0018 Train Accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Validation Loss:     1.4537 Validation Accuracy: 0.739400\n",
      "Train Loss:     0.0015 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Validation Loss:     1.5676 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0046 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 2:  Validation Loss:     1.5199 Validation Accuracy: 0.731800\n",
      "Train Loss:     0.0016 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Validation Loss:     1.6083 Validation Accuracy: 0.707400\n",
      "Train Loss:     0.0062 Train Accuracy: 0.998762\n",
      "Epoch 80, CIFAR-10 Batch 4:  Validation Loss:     1.4934 Validation Accuracy: 0.724400\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Validation Loss:     1.5028 Validation Accuracy: 0.725000\n",
      "Train Loss:     0.0015 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  Validation Loss:     1.6054 Validation Accuracy: 0.724400\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 2:  Validation Loss:     1.5355 Validation Accuracy: 0.718600\n",
      "Train Loss:     0.0058 Train Accuracy: 0.997525\n",
      "Epoch 81, CIFAR-10 Batch 3:  Validation Loss:     1.7177 Validation Accuracy: 0.704200\n",
      "Train Loss:     0.0046 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 4:  Validation Loss:     1.5576 Validation Accuracy: 0.718000\n",
      "Train Loss:     0.0021 Train Accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 5:  Validation Loss:     1.5553 Validation Accuracy: 0.725200\n",
      "Train Loss:     0.0024 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 1:  Validation Loss:     1.6048 Validation Accuracy: 0.713400\n",
      "Train Loss:     0.0033 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 2:  Validation Loss:     1.5581 Validation Accuracy: 0.720600\n",
      "Train Loss:     0.0056 Train Accuracy: 0.998762\n",
      "Epoch 82, CIFAR-10 Batch 3:  Validation Loss:     1.5735 Validation Accuracy: 0.713800\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 4:  Validation Loss:     1.5205 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 5:  Validation Loss:     1.5041 Validation Accuracy: 0.734800\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Validation Loss:     1.5714 Validation Accuracy: 0.724200\n",
      "Train Loss:     0.0029 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 2:  Validation Loss:     1.6108 Validation Accuracy: 0.715200\n",
      "Train Loss:     0.0050 Train Accuracy: 0.998762\n",
      "Epoch 83, CIFAR-10 Batch 3:  Validation Loss:     1.6350 Validation Accuracy: 0.715400\n",
      "Train Loss:     0.0042 Train Accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 4:  Validation Loss:     1.5634 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0026 Train Accuracy: 0.998762\n",
      "Epoch 83, CIFAR-10 Batch 5:  Validation Loss:     1.5037 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0024 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  Validation Loss:     1.5924 Validation Accuracy: 0.718200\n",
      "Train Loss:     0.0045 Train Accuracy: 0.998762\n",
      "Epoch 84, CIFAR-10 Batch 2:  Validation Loss:     1.5392 Validation Accuracy: 0.723400\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 3:  Validation Loss:     1.6148 Validation Accuracy: 0.724000\n",
      "Train Loss:     0.0028 Train Accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 4:  Validation Loss:     1.6585 Validation Accuracy: 0.715400\n",
      "Train Loss:     0.0046 Train Accuracy: 0.998762\n",
      "Epoch 84, CIFAR-10 Batch 5:  Validation Loss:     1.5790 Validation Accuracy: 0.719400\n",
      "Train Loss:     0.0037 Train Accuracy: 0.998762\n",
      "Epoch 85, CIFAR-10 Batch 1:  Validation Loss:     1.6000 Validation Accuracy: 0.725000\n",
      "Train Loss:     0.0037 Train Accuracy: 0.998762\n",
      "Epoch 85, CIFAR-10 Batch 2:  Validation Loss:     1.4911 Validation Accuracy: 0.730000\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 3:  Validation Loss:     1.5354 Validation Accuracy: 0.724800\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 4:  Validation Loss:     1.5509 Validation Accuracy: 0.716600\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 5:  Validation Loss:     1.5073 Validation Accuracy: 0.722000\n",
      "Train Loss:     0.0031 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  Validation Loss:     1.5177 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0052 Train Accuracy: 0.998762\n",
      "Epoch 86, CIFAR-10 Batch 2:  Validation Loss:     1.4658 Validation Accuracy: 0.723400\n",
      "Train Loss:     0.0034 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, CIFAR-10 Batch 3:  Validation Loss:     1.6473 Validation Accuracy: 0.712200\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 4:  Validation Loss:     1.5905 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0040 Train Accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 5:  Validation Loss:     1.5695 Validation Accuracy: 0.729600\n",
      "Train Loss:     0.0032 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Validation Loss:     1.5552 Validation Accuracy: 0.724400\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 2:  Validation Loss:     1.5302 Validation Accuracy: 0.719400\n",
      "Train Loss:     0.0032 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Validation Loss:     1.6197 Validation Accuracy: 0.721400\n",
      "Train Loss:     0.0022 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 4:  Validation Loss:     1.6438 Validation Accuracy: 0.712400\n",
      "Train Loss:     0.0036 Train Accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Validation Loss:     1.6003 Validation Accuracy: 0.721600\n",
      "Train Loss:     0.0023 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Validation Loss:     1.5441 Validation Accuracy: 0.725400\n",
      "Train Loss:     0.0040 Train Accuracy: 0.998762\n",
      "Epoch 88, CIFAR-10 Batch 2:  Validation Loss:     1.5459 Validation Accuracy: 0.722000\n",
      "Train Loss:     0.0021 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Validation Loss:     1.6948 Validation Accuracy: 0.717200\n",
      "Train Loss:     0.0067 Train Accuracy: 0.998762\n",
      "Epoch 88, CIFAR-10 Batch 4:  Validation Loss:     1.6053 Validation Accuracy: 0.719200\n",
      "Train Loss:     0.0038 Train Accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 5:  Validation Loss:     1.5445 Validation Accuracy: 0.726400\n",
      "Train Loss:     0.0025 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  Validation Loss:     1.5525 Validation Accuracy: 0.725400\n",
      "Train Loss:     0.0015 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 2:  Validation Loss:     1.5641 Validation Accuracy: 0.730400\n",
      "Train Loss:     0.0021 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 3:  Validation Loss:     1.6075 Validation Accuracy: 0.720200\n",
      "Train Loss:     0.0035 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 4:  Validation Loss:     1.6155 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0025 Train Accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Validation Loss:     1.6267 Validation Accuracy: 0.718200\n",
      "Train Loss:     0.0050 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  Validation Loss:     1.4879 Validation Accuracy: 0.729400\n",
      "Train Loss:     0.0024 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Validation Loss:     1.5060 Validation Accuracy: 0.729200\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 3:  Validation Loss:     1.5827 Validation Accuracy: 0.725600\n",
      "Train Loss:     0.0029 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 4:  Validation Loss:     1.6698 Validation Accuracy: 0.714600\n",
      "Train Loss:     0.0053 Train Accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Validation Loss:     1.5474 Validation Accuracy: 0.730200\n",
      "Train Loss:     0.0030 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Validation Loss:     1.5030 Validation Accuracy: 0.730600\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Validation Loss:     1.5897 Validation Accuracy: 0.728200\n",
      "Train Loss:     0.0020 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 3:  Validation Loss:     1.6441 Validation Accuracy: 0.724000\n",
      "Train Loss:     0.0017 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 4:  Validation Loss:     1.5777 Validation Accuracy: 0.719200\n",
      "Train Loss:     0.0016 Train Accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Validation Loss:     1.5967 Validation Accuracy: 0.726200\n",
      "Train Loss:     0.0046 Train Accuracy: 0.998762\n",
      "Epoch 92, CIFAR-10 Batch 1:  Validation Loss:     1.5570 Validation Accuracy: 0.730400\n",
      "Train Loss:     0.0033 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 2:  Validation Loss:     1.6458 Validation Accuracy: 0.720600\n",
      "Train Loss:     0.0089 Train Accuracy: 0.997525\n",
      "Epoch 92, CIFAR-10 Batch 3:  Validation Loss:     1.6613 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0028 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 4:  Validation Loss:     1.6088 Validation Accuracy: 0.728400\n",
      "Train Loss:     0.0023 Train Accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Validation Loss:     1.6266 Validation Accuracy: 0.729000\n",
      "Train Loss:     0.0022 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  Validation Loss:     1.5799 Validation Accuracy: 0.733800\n",
      "Train Loss:     0.0010 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 2:  Validation Loss:     1.6218 Validation Accuracy: 0.730400\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 3:  Validation Loss:     1.6175 Validation Accuracy: 0.730600\n",
      "Train Loss:     0.0009 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Validation Loss:     1.6377 Validation Accuracy: 0.725400\n",
      "Train Loss:     0.0016 Train Accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 5:  Validation Loss:     1.6299 Validation Accuracy: 0.721000\n",
      "Train Loss:     0.0045 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Validation Loss:     1.6130 Validation Accuracy: 0.732800\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 2:  Validation Loss:     1.6208 Validation Accuracy: 0.731600\n",
      "Train Loss:     0.0029 Train Accuracy: 0.998762\n",
      "Epoch 94, CIFAR-10 Batch 3:  Validation Loss:     1.7419 Validation Accuracy: 0.723200\n",
      "Train Loss:     0.0034 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 4:  Validation Loss:     1.6921 Validation Accuracy: 0.722400\n",
      "Train Loss:     0.0018 Train Accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 5:  Validation Loss:     1.6510 Validation Accuracy: 0.725600\n",
      "Train Loss:     0.0024 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  Validation Loss:     1.6703 Validation Accuracy: 0.728000\n",
      "Train Loss:     0.0014 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 2:  Validation Loss:     1.6457 Validation Accuracy: 0.727200\n",
      "Train Loss:     0.0032 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Validation Loss:     1.7065 Validation Accuracy: 0.726800\n",
      "Train Loss:     0.0015 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 4:  Validation Loss:     1.6513 Validation Accuracy: 0.726600\n",
      "Train Loss:     0.0006 Train Accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Validation Loss:     1.6607 Validation Accuracy: 0.720400\n",
      "Train Loss:     0.0012 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  Validation Loss:     1.6200 Validation Accuracy: 0.730200\n",
      "Train Loss:     0.0021 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 2:  Validation Loss:     1.5990 Validation Accuracy: 0.728600\n",
      "Train Loss:     0.0022 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 3:  Validation Loss:     1.7162 Validation Accuracy: 0.723800\n",
      "Train Loss:     0.0024 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 4:  Validation Loss:     1.7164 Validation Accuracy: 0.718000\n",
      "Train Loss:     0.0010 Train Accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 5:  Validation Loss:     1.6747 Validation Accuracy: 0.732000\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Validation Loss:     1.5861 Validation Accuracy: 0.735800\n",
      "Train Loss:     0.0011 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 2:  Validation Loss:     1.6444 Validation Accuracy: 0.727400\n",
      "Train Loss:     0.0018 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 3:  Validation Loss:     1.6502 Validation Accuracy: 0.735000\n",
      "Train Loss:     0.0007 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 4:  Validation Loss:     1.6793 Validation Accuracy: 0.722400\n",
      "Train Loss:     0.0011 Train Accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 5:  Validation Loss:     1.6824 Validation Accuracy: 0.718800\n",
      "Train Loss:     0.0016 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 1:  Validation Loss:     1.6326 Validation Accuracy: 0.730000\n",
      "Train Loss:     0.0009 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 2:  Validation Loss:     1.6587 Validation Accuracy: 0.725000\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 3:  Validation Loss:     1.7211 Validation Accuracy: 0.722600\n",
      "Train Loss:     0.0015 Train Accuracy: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, CIFAR-10 Batch 4:  Validation Loss:     1.6614 Validation Accuracy: 0.721800\n",
      "Train Loss:     0.0032 Train Accuracy: 0.998762\n",
      "Epoch 98, CIFAR-10 Batch 5:  Validation Loss:     1.6515 Validation Accuracy: 0.724400\n",
      "Train Loss:     0.0010 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Validation Loss:     1.6409 Validation Accuracy: 0.728200\n",
      "Train Loss:     0.0007 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 2:  Validation Loss:     1.5958 Validation Accuracy: 0.725800\n",
      "Train Loss:     0.0013 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 3:  Validation Loss:     1.6876 Validation Accuracy: 0.724600\n",
      "Train Loss:     0.0014 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 4:  Validation Loss:     1.6376 Validation Accuracy: 0.726000\n",
      "Train Loss:     0.0011 Train Accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 5:  Validation Loss:     1.7460 Validation Accuracy: 0.719200\n",
      "Train Loss:     0.0026 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  Validation Loss:     1.7068 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0019 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 2:  Validation Loss:     1.6461 Validation Accuracy: 0.729800\n",
      "Train Loss:     0.0014 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 3:  Validation Loss:     1.6601 Validation Accuracy: 0.729400\n",
      "Train Loss:     0.0010 Train Accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 4:  Validation Loss:     1.6659 Validation Accuracy: 0.726000\n",
      "Train Loss:     0.0019 Train Accuracy: 0.998762\n",
      "Epoch 100, CIFAR-10 Batch 5:  Validation Loss:     1.6370 Validation Accuracy: 0.723600\n",
      "Train Loss:     0.0025 Train Accuracy: 0.998762\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7221719563007355\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31e5347a20>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
